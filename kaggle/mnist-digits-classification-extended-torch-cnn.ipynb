{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63537188-e57e-40c1-a82c-844be261375c"
   },
   "source": [
    "# MNIST Digits Classification - Extended PyTorchðŸ”¥CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences to the \"Simple PyTorchðŸ”¥CNN\" Notebook: \n",
    "- More complex network (more convoluted layers, more filters)\n",
    "- As CUDA runs into memory problems on Kaggle when predicting a whole train, validation, or test dataset, we need to use batches to compute the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc8f4523-57da-4bf5-8a6a-65f2b563eda3"
   },
   "source": [
    "Subject: Building a CNN Classificator with PyTorch to classify hand-written digits.\n",
    "\n",
    "Data: MNIST (handwritten digits) via torchvision\n",
    "\n",
    "Procedure:\n",
    "- Previewing images from dataset with pyplot's imshow()\n",
    "- Neural network with torch.nn.module, torch.nn.Sequential, torch.nn.Conv2d, torch.nn.ReLU, torch.nn.MaxPool2d, torch.nn.Dropout, torch.nn.Linear, and torch.nn.init.xavier_uniform_\n",
    "- Visualizing model with torchviz' make_dot()\n",
    "- Training with nn.CrossEntropyLoss and torch.optim.Adam optimizer\n",
    "- Visualization Loss and Accuracy with pyplot\n",
    "- Visualization of the CNN Layers with pyplot's imshow()\n",
    "- Good results\n",
    "\n",
    "Others:\n",
    "- Compatible with Google Colab and Kaggle as runtime\n",
    "- CUDA support\n",
    "\n",
    "Sources used:\n",
    "- https://machinelearningknowledge.ai/pytorch-conv2d-explained-with-examples/\n",
    "- https://www.kaggle.com/code/raghaw/mnist-cnn-external-data-perfect-score\n",
    "- Probably some more, but this is an old Notebook and I forgot the sources. Please let me know if I copied your code and I will mention it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a8ae951-ac9f-49f6-95a9-5278bb3b1d7f"
   },
   "source": [
    "## Bootstrap and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a2707413-c690-47d9-8653-faaa579805ad",
    "outputId": "74f54c7f-58ec-40e7-d543-5cb2e74224aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {DEVICE}')\n",
    "\n",
    "# running in google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    NUM_EPOCHS = 50\n",
    "    !pip install torchviz\n",
    "    BASE_PATH = './drive/MyDrive/Colab/data/'\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# running interactively in kaggle\n",
    "elif get_ipython().config.IPKernelApp.connection_file.startswith('/root/.local/share'):\n",
    "    NUM_EPOCHS = 5\n",
    "    BASE_PATH = '/kaggle/input/'\n",
    "    !pip install torchviz\n",
    "    \n",
    "# running as background job in kaggle\n",
    "elif 'SHLVL' in os.environ:\n",
    "    NUM_EPOCHS = 50\n",
    "    BASE_PATH = '/kaggle/input/'\n",
    "    !pip install torchviz\n",
    "\n",
    "else:\n",
    "    BASE_PATH = '../data/'\n",
    "    NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0d009e82-fd22-47ba-88db-405d5e6736f6",
    "outputId": "99f83002-7c34-4024-9a98-a00db2a23b34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x297281b5e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from collections.abc import Callable\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, locale='')  # for thousands separator via ... print(f'{value:n}')\"\n",
    "import math\n",
    "from itertools import islice\n",
    "from collections.abc import Iterable, Generator\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "import time\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sklearn.metrics\n",
    "\n",
    "my_seed = 123\n",
    "random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df5a0ac4-29c4-4c40-8fee-33f53780f076"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/digit-recognizer/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m path_train \u001b[38;5;241m=\u001b[39m BASE_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigit-recognizer/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m path_test \u001b[38;5;241m=\u001b[39m BASE_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigit-recognizer/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df_train_source \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path_test)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_train_source\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\notebooks-JGOOFdU--py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\notebooks-JGOOFdU--py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\notebooks-JGOOFdU--py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\notebooks-JGOOFdU--py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\notebooks-JGOOFdU--py3.11\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/digit-recognizer/train.csv'"
     ]
    }
   ],
   "source": [
    "path_train = BASE_PATH + 'digit-recognizer/train.csv'\n",
    "path_test = BASE_PATH + 'digit-recognizer/test.csv'\n",
    "\n",
    "df_train_source = pd.read_csv(path_train)\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "print(df_train_source.columns)\n",
    "print(df_test.columns)\n",
    "\n",
    "print(df_train_source.shape)\n",
    "print(df_test.shape)\n",
    "assert 'label' in df_train_source\n",
    "assert 'label' not in df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into train and validation\n",
    "df_train_randomized = df_train_source.sample(frac=1)\n",
    "NUM_VAL = int(len(df_train_randomized) * 0.15)\n",
    "\n",
    "df_train_with_label = df_train_randomized[:-NUM_VAL]  # (35700, 785)\n",
    "df_val_with_label = df_train_randomized[-NUM_VAL:]  # (6300, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_with_label.iloc[:,1:]  # (35700, 784)\n",
    "df_val = df_val_with_label.iloc[:,1:]  # (6300, 784)\n",
    "\n",
    "ser_y_train = df_train_with_label.iloc[:,0]  # (35700,)\n",
    "ser_y_val = df_val_with_label.iloc[:,0]  # (6300,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df_train.columns == df_val.columns)\n",
    "assert all(df_train.columns == df_test.columns)\n",
    "\n",
    "# no nan treatment required\n",
    "assert df_train.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T10:49:41.388714Z",
     "iopub.status.busy": "2023-09-18T10:49:41.388271Z",
     "iopub.status.idle": "2023-09-18T10:49:41.415270Z",
     "shell.execute_reply": "2023-09-18T10:49:41.414290Z",
     "shell.execute_reply.started": "2023-09-18T10:49:41.388678Z"
    }
   },
   "source": [
    "## Tensorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flat 784 (1..255) to 28*28 pixels with 1 normalized channel (0.0..1.0)\n",
    "def reshape(df: pd.DataFrame) -> np.array:  # df: (n, 784), all int64\n",
    "    df = df.values.reshape(-1, 28, 28)  # (n, 28, 28), int64\n",
    "    df = df.astype(np.float32)  # (n, 28, 28), float32\n",
    "    # the pixels have 256 values (0.0..255.0), therefore we normalize to (0.0..1.0)\n",
    "    df = df / 255.0\n",
    "    return df\n",
    "    \n",
    "    \n",
    "arr_train = reshape(df_train)  # np.array (35700, 28, 28), float32\n",
    "arr_val = reshape(df_val)  # np.array (6300, 28, 28)\n",
    "arr_test = reshape(df_test)  # np.array (28000, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorize\n",
    "train = torch.tensor(arr_train).to(DEVICE)  # torch.float32\n",
    "val = torch.tensor(arr_val).to(DEVICE)\n",
    "test = torch.tensor(arr_test).to(DEVICE)\n",
    "\n",
    "y_train = torch.tensor(ser_y_train.values).to(DEVICE)  # [35700], torch.int64\n",
    "y_val = torch.tensor(ser_y_val.values).to(DEVICE)  # [6300], torch.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(train, y_train)\n",
    "dataset_val = TensorDataset(val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(dataset=dataset_train,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(dataset=dataset_val,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview first ten images\n",
    "def print_ten_numbers(x: torch.Tensor, \n",
    "                      y: torch.Tensor):\n",
    "    fig = plt.figure(figsize=(25, 10))  # (width, height) in inches\n",
    "    for i in range(10):\n",
    "        ax = fig.add_subplot(1,  # nrows\n",
    "                             10,  # ncols\n",
    "                             i+1, # index (1-based)\n",
    "                             xticks=[],\n",
    "                             yticks=[])\n",
    "        image = x[i]  # [28, 28]\n",
    "        label = y[i].item()  # int\n",
    "        ax.imshow(X=image.squeeze().cpu(),\n",
    "                  cmap='gray')\n",
    "        ax.set_title(f\"{label}\")\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))   # x_batch: [128, 28, 28] torch.float32, y_batch: [128] torch.int64\n",
    "print_ten_numbers(x=x_batch, y=y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ab5f6f3-4749-478c-b511-77b07a97a6ff"
   },
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(torch.nn.Module):\n",
    "    # https://machinelearningknowledge.ai/pytorch-conv2d-explained-with-examples/\n",
    "\n",
    "    def __init__(self, dropout_probability=0.3):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # add dummy input channel: [batch_size, 28, 28] -> [batch_size, 1, 28, 28]\n",
    "        self.unflatten = torch.nn.Unflatten(dim=1, unflattened_size=(1,28))\n",
    "        \n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            # (batch_size, 1, 28, 28) -> (batch_size, 64, 28, 28)\n",
    "            torch.nn.Conv2d(in_channels=1,  # Number of channels in the input image\n",
    "                            out_channels=64,  # Number of channels produced by the convolution\n",
    "                            kernel_size=3, #  Size of the convolving kernel\n",
    "                            stride=1,  # Stride of the convolution. Default: 1\n",
    "                            padding=1,  # Padding added to all four sides of the input. Default: 0\n",
    "                           ),\n",
    "            # (element-wise)\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # shape unchanged\n",
    "            torch.nn.Conv2d(in_channels=64,\n",
    "                            out_channels=64,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # [batch_size, 64, 28, 28] -> [batch_size, 128, 28, 28]\n",
    "            torch.nn.Conv2d(in_channels=64,\n",
    "                            out_channels=128,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # [batch_size, 128, 28, 28] -> [batch_size, 128, 14, 14]\n",
    "            torch.nn.MaxPool2d(kernel_size=2, # the size of the window to take a max over\n",
    "                               stride=2,  # the stride of the window. Default value is kernel_size\n",
    "                              ),\n",
    "            # (element-wise)\n",
    "            torch.nn.Dropout(p=dropout_probability,  # probability of an element to be zeroed. Default: 0.5\n",
    "                            ),\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            # shape unchanged\n",
    "            torch.nn.Conv2d(in_channels=128,\n",
    "                            out_channels=128,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # [batch_size, 128, 14, 14] -> [batch_size, 192, 14, 14]\n",
    "            torch.nn.Conv2d(in_channels=128,\n",
    "                            out_channels=192,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # [batch_size, 192, 14, 14] -> [batch_size, 192, 7, 7]\n",
    "            torch.nn.MaxPool2d(kernel_size=2,\n",
    "                               stride=2),\n",
    "            torch.nn.Dropout(p=dropout_probability))\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            # shape unchanged\n",
    "            torch.nn.Conv2d(in_channels=192,\n",
    "                            out_channels=192,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # [batch_size, 192, 7, 7] --> [batch_size, 192, 4, 4]\n",
    "            torch.nn.MaxPool2d(kernel_size=2, \n",
    "                               stride=2, \n",
    "                               padding=1),  # default: 0\n",
    "            torch.nn.Dropout(p=dropout_probability)\n",
    "            )\n",
    "        \n",
    "        # [batch_size, 192, 4, 4] -> [batch_size, 2048]\n",
    "        self.flatten = torch.nn.Flatten()  # for feed-forward \n",
    "\n",
    "        # [batch_size, 2048] --> [batch_size, 625]\n",
    "        self.fc1 = torch.nn.Linear(in_features=4 * 4 * 192,\n",
    "                                   out_features=625,\n",
    "                                   bias=True)\n",
    "        \n",
    "        # [batch_size, 625] --> [batch_size, 10]\n",
    "        self.fc2 = torch.nn.Linear(in_features=625,\n",
    "                                   out_features=10,\n",
    "                                   bias=True)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)  # initialize weights (seems to make no difference in scores)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight) \n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x: [batch_size, 28, 28]\n",
    "        \n",
    "        # CNN\n",
    "        x_unflattened = self.unflatten(x)  # [batch_size, 1, 28, 28]\n",
    "        output_layer_1 = self.layer1(x_unflattened)  # [batch_size, 128, 14, 14]\n",
    "        output_layer_2 = self.layer2(output_layer_1)  # [batch_size, 192, 7, 7]\n",
    "        output_layer_3 = self.layer3(output_layer_2)  # [batch_size, 192, 4, 4]\n",
    "        flattened = self.flatten(output_layer_3)  # flattened to[batch_size, 3072]\n",
    "        \n",
    "        # FC\n",
    "        output_fully_connected_1 = self.fc1(flattened)  # [batch_size, 625]\n",
    "        output_fully_connected_2 = self.fc2(output_fully_connected_1)  # [batch_size, 10]\n",
    "\n",
    "        return output_fully_connected_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2903f806-70e6-44ca-90bc-f83486d89692",
    "outputId": "5811b562-5fdc-4075-e4dd-a1ce20b9a44a"
   },
   "outputs": [],
   "source": [
    "# visualize the classifier\n",
    "c_temp = CNNClassifier().to(DEVICE)\n",
    "# to visualize with torchviz, we need some input that can pass through the model's forward() method.\n",
    "predictions = c_temp(x_batch)\n",
    "make_dot(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8226b58-ecfd-4803-b2fc-596d78f72817"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6846e1a8-e39e-43cd-97e9-3c5415a531f3"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "classifier = CNNClassifier().to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(),\n",
    "                             lr = LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  # reduce learning rate when model stops improving on validation dataset \n",
    "                                                       mode='min',\n",
    "                                                       factor=0.3,\n",
    "                                                       patience=2,\n",
    "                                                       min_lr=0.00000001,\n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(classifier: CNNClassifier, \n",
    "                    loss_fn: Callable,\n",
    "                    dataloader: torch.utils.data.DataLoader,\n",
    "                   )->tuple[float, float, float]:\n",
    "        \n",
    "        losses = []\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        \n",
    "        for x_batch, y_batch in dataloader:\n",
    "            pred_logits_batch = classifier(x_batch)  # [batch_size, 10]\n",
    "            loss = loss_fn(pred_logits_batch,\n",
    "                           y_batch)\n",
    "            losses.extend([loss.item()] * len(x_batch))\n",
    "            \n",
    "            pred_batch = pred_logits_batch.argmax(dim=1)  # [batch_size]\n",
    "            predictions.extend(pred_batch.tolist())\n",
    "            y_true.extend(y_batch.tolist())\n",
    "        \n",
    "        avg_loss = np.mean(losses)\n",
    "        correct = (np.array(predictions) == np.array(y_true))\n",
    "        accuracy = correct.mean()\n",
    "        f1_score = sklearn.metrics.f1_score(y_true=y_true, \n",
    "                                            y_pred=predictions,\n",
    "                                            average='micro')  # multi-class problem\n",
    "        \n",
    "        return avg_loss, accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42479bc0-e113-4984-8de1-c48b69c4428b",
    "outputId": "d0a60f95-3cdf-48d9-c783-66a17f86fc0a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(columns=['loss_train', 'accuracy_train', 'f1_train', \n",
    "                                   'loss_val', 'accuracy_val', 'f1_val'],\n",
    "                          index=range(NUM_EPOCHS))\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for batch, (x_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        # x_train_batch: [batch_size, 28, 28] torch.float32\n",
    "        # y_train_batch: [batch_size] torch.int64\n",
    "\n",
    "        # switch to training mode mode (we might have been in evaluation mode)\n",
    "        classifier.train()\n",
    "\n",
    "        pred_train_batch = classifier(x_train_batch)  # [batch_size, 10]\n",
    "\n",
    "        # clear existing gradients from previous batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(pred_train_batch,\n",
    "                       y_train_batch)  # [], .item() is e.g. 2.291177988052368\n",
    "\n",
    "        # compute gradients (backpropagation), then apply gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # after each epoch, switch to evaluation mode, then evaluate without computing gradients\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_train, accuracy_train, f1_score_train = compute_metrics(classifier, loss_fn, train_loader)\n",
    "        loss_val, accuracy_val, f1_score_val = compute_metrics(classifier, loss_fn, val_loader)\n",
    "\n",
    "        df_metrics.iloc[epoch] = [loss_train, accuracy_train, f1_score_train,\n",
    "                                  loss_val, accuracy_val, f1_score_val]\n",
    "        \n",
    "    scheduler.step(loss_val)\n",
    "    print(f'Accuracy Validation after epoch {epoch}: {accuracy_val :.4f}  '\n",
    "          f'(Train: {accuracy_train :.4f}) '\n",
    "          f'LR = {optimizer.param_groups[0][\"lr\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(NUM_EPOCHS)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, _)) = plt.subplots(nrows=2,\n",
    "                                       ncols=2,\n",
    "                                       figsize=(15,5),\n",
    "                                          sharex=True)\n",
    "\n",
    "# Plot and label the training and val loss values\n",
    "ax1.plot(epochs, df_metrics['loss_train'], label='Training Loss')\n",
    "ax1.plot(epochs, df_metrics['loss_val'], label='val Loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "# ... Accuracy\n",
    "ax2.plot(epochs, df_metrics['accuracy_train'], label='Training Accuracy')\n",
    "ax2.plot(epochs, df_metrics['accuracy_val'], label='val Accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "# ... F1-Score\n",
    "ax3.plot(epochs, df_metrics['f1_train'], label='Training F1-Score')\n",
    "ax3.plot(epochs, df_metrics['f1_val'], label='val F1-Score')\n",
    "ax3.set_ylabel('F1-Score')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_xticks(np.arange(0, \n",
    "                         NUM_EPOCHS))\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0, \n",
    "                     NUM_EPOCHS))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at some of the misclassified images from the validation dataset\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = classifier(val)\n",
    "    y_pred = y_pred_logits.argmax(dim=1)\n",
    "    correct = (y_pred == y_val).type(torch.FloatTensor)  # [6300] with either 1.0 or 0.0\n",
    "   \n",
    "ser_correct = pd.Series(correct)\n",
    "print(ser_correct.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indexes = ser_correct[ser_correct == 0.0].index.to_list()\n",
    "\n",
    "val_misclassified = val[bad_indexes]  # [69, 28, 28]\n",
    "y_val_misclassified = y_val[bad_indexes]\n",
    "y_pred_misclassified = y_pred[bad_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=math.ceil(len(val_misclassified) / 10),  # axes: np.array of shape (7, 10)\n",
    "                         ncols=10,\n",
    "                         figsize=(15,15),\n",
    "                        )\n",
    "\n",
    "for i in range(len(val_misclassified)):\n",
    "    image = val_misclassified[i]\n",
    "    y = y_val_misclassified[i].item()\n",
    "    pred = y_pred_misclassified[i].item()\n",
    "    \n",
    "    ax = axes[i//10, i%10]\n",
    "    \n",
    "    ax.imshow(X=image.squeeze(dim=0).cpu(),\n",
    "              cmap='gray')\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([]) \n",
    "    ax.set_title(f\"âœ“{y} / âš  {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the current model, predicting the whole test dataset in one batch\n",
    "# runs into cuda memory problems\n",
    "test.shape  # [28000, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunks: tuple[torch.Tensor, ...] = torch.split(test, 1000)\n",
    "\n",
    "predicted_labels: list[int] = []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for test_chunk in test_chunks:\n",
    "        pred_logits = classifier(test_chunk)  # [chunk_size, 10]\n",
    "        pred = pred_logits.argmax(dim=1)   # [chunk_size]\n",
    "        predicted_labels.extend(pred.tolist())\n",
    "\n",
    "assert len(predicted_labels) == len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'ImageId': range(1, len(test)+1),\n",
    "                        'Label': predicted_labels})\n",
    "df_pred['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv('submission.csv',\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86ef8fc9-4c4f-481f-ae7d-27cada9d49ef"
   },
   "source": [
    "# Visualization of CNN Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d81f195-4a6b-4de2-a03e-e9c71b551559",
    "outputId": "9a8af4bf-bef8-4790-9238-f0dbc9ed0d7c"
   },
   "outputs": [],
   "source": [
    "n_example = 100\n",
    "\n",
    "# let's look at an example image from the validation dataset:\n",
    "x_example = val[n_example].detach()  # [28, 28]\n",
    "plt.imshow(x_example.cpu(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89e2475b-c025-4019-bbe9-122de8718596",
    "outputId": "92445aec-5219-4191-c006-d6248bad0b7f"
   },
   "outputs": [],
   "source": [
    "# predicted logits\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_logits = classifier(x_example.unsqueeze(dim=0).to(DEVICE))  # [1, 10]\n",
    "    for i in range(10):\n",
    "        print(f'{i}: {round(predicted_logits[0][i].item(), 2) :5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97ef96c4-23c2-43e7-97b1-906f709467ee"
   },
   "source": [
    "## Outputs of CNN Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24d69dfc-df4d-4922-afd1-cec4c9037ce4",
    "outputId": "c1d13fe0-cd45-4e19-c266-5ed759f83aac"
   },
   "outputs": [],
   "source": [
    "# from [28, 28] to [batch_size, channel, 28, 28], i.e. [1, 1, 28, 28]\n",
    "x_unsqueezed = x_example.unsqueeze(dim=0)\n",
    "x_unsqueezed = x_unsqueezed.unsqueeze(dim=0)\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    output_layer_1 = classifier.layer1(x_unsqueezed.to(DEVICE).detach())  # 1, 128, 14, 14\n",
    "    print(output_layer_1.shape)\n",
    "                                       \n",
    "\n",
    "fig = plt.figure(figsize=(25, 10))  # (width, height) in inches\n",
    "for i in range(40):  # can't show all 128, 40 is the limit here\n",
    "    ax = fig.add_subplot(4,         # nrows\n",
    "                         10,        # ncols\n",
    "                         i+1,       # index (1-based)\n",
    "                         xticks=[],\n",
    "                         yticks=[])\n",
    "    image = output_layer_1[0][i]  # (14, 14)\n",
    "    ax.imshow(X=image.cpu(),\n",
    "              cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abc483b0-4a95-4443-84d5-db3138a2f616"
   },
   "source": [
    "### Show the outputs of CNN Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cecabc7-f944-494f-9fbe-24643d4280f7",
    "outputId": "362795b9-0ab0-4686-ebb1-afd5577eb75f"
   },
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    output_layer_2 = classifier.layer2(output_layer_1.to(DEVICE)).detach()  # [1, 192, 7, 7]\n",
    "\n",
    "fig = plt.figure(figsize=(25, 5))  # (width, height) in inches\n",
    "for i in range(80):\n",
    "    ax = fig.add_subplot(\n",
    "                 4,         # nrows\n",
    "                 20,        # ncols\n",
    "                 i+1,       # index (1-based)\n",
    "                 xticks=[],\n",
    "                 yticks=[])\n",
    "    image = output_layer_2[0][i]  # (7, 7)\n",
    "    ax.imshow(X=image.cpu(),\n",
    "              cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6440edfb-98e1-4858-b972-548430104c32"
   },
   "source": [
    "### Show the outputs of CNN Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25f9ddeb-066a-47f9-ad29-b709207e9bfb",
    "outputId": "4defba48-4be9-49f4-8fbd-1681c574ccd1"
   },
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    output_layer_3 = classifier.layer3(output_layer_2.to(DEVICE)).detach()  # [1, 192, 4, 4]\n",
    "    \n",
    "fig = plt.figure(figsize=(25, 5))  # (width, height) in inches\n",
    "for i in range(150):\n",
    "    ax = fig.add_subplot(\n",
    "                 5,         # nrows\n",
    "                 30,        # ncols\n",
    "                 i+1,       # index (1-based)\n",
    "                 xticks=[],\n",
    "                 yticks=[])\n",
    "    image = output_layer_3[0][i]  # (28, 28)\n",
    "    ax.imshow(X=image.cpu(),\n",
    "              cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2548b830-0b64-4538-bbb4-0a285d5b0c9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
