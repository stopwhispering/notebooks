{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Disaster Tweets - EDA","metadata":{}},{"cell_type":"markdown","source":"Subject: Performing EDA for Twitter Disaster Tweets Dataset\n\nData: Twitter Disaster Tweets Dataset (NLP getting started dataset) via Kaggle\n(https://www.kaggle.com/competitions/nlp-getting-started/data)\n\nProcedure:\n- Load files, replace NaNs\n- Analyze and discard keyword and target features\n- Display Target Variable Balance\n- Analyze Text Length (Characters, Words, Punctuation, Stopwords)\n- Analyze most frequent words\n- Analyze bigrams and trigrams \n\nOthers:\n- Compatible with Google Colab and Kaggle as runtime\n\nSources used:\n- https://www.kaggle.com/code/colearninglounge/nlp-data-preprocessing-and-cleaning/notebook?scriptVersionId=48903343\n- https://www.kaggle.com/code/yakinoki/natural-language-processing-with-disaster-tweets\n- https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert/notebook","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Running on {DEVICE}')\n\n# running in google colab\nif 'google.colab' in str(get_ipython()):\n    BASE_PATH = './drive/MyDrive/Colab/data/'\n    from google.colab import drive\n    drive.mount('/content/drive')\n    import nltk\n    nltk.download('stopwords')\n\n# running interactively in kaggle\nelif get_ipython().config.IPKernelApp.connection_file.startswith('/root/.local/share'):\n    BASE_PATH = '/kaggle/input/'\n    \n# running as background job in kaggle\nelif 'SHLVL' in os.environ:\n    BASE_PATH = '/kaggle/input/'\n\nelse:\n    BASE_PATH = '../data/'","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:32:42.842687Z","iopub.execute_input":"2023-09-17T10:32:42.843070Z","iopub.status.idle":"2023-09-17T10:32:46.870659Z","shell.execute_reply.started":"2023-09-17T10:32:42.843041Z","shell.execute_reply":"2023-09-17T10:32:46.869283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport pprint\nimport string\nfrom collections import Counter, defaultdict\nimport locale\nlocale.setlocale(locale.LC_ALL, locale='')  # for thousands separator via ... print(f'{value:n}')\"\n\nimport pandas as pd\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.ticker\nfrom matplotlib.axes._axes import Axes\nfrom wordcloud import WordCloud\nimport nltk\n\nmy_seed = 42\nrandom.seed(my_seed)\ntorch.manual_seed(my_seed);","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:32:46.874527Z","iopub.execute_input":"2023-09-17T10:32:46.875012Z","iopub.status.idle":"2023-09-17T10:32:49.544092Z","shell.execute_reply.started":"2023-09-17T10:32:46.874980Z","shell.execute_reply":"2023-09-17T10:32:49.543171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df_train_source = pd.read_csv(BASE_PATH + 'nlp-getting-started/train.csv')\ndf_test_source = pd.read_csv(BASE_PATH + 'nlp-getting-started/test.csv')\ndf_train_source","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:32:49.545384Z","iopub.execute_input":"2023-09-17T10:32:49.546379Z","iopub.status.idle":"2023-09-17T10:32:49.656166Z","shell.execute_reply.started":"2023-09-17T10:32:49.546344Z","shell.execute_reply":"2023-09-17T10:32:49.655027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_source.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:32:49.658958Z","iopub.execute_input":"2023-09-17T10:32:49.659327Z","iopub.status.idle":"2023-09-17T10:32:49.666367Z","shell.execute_reply.started":"2023-09-17T10:32:49.659296Z","shell.execute_reply":"2023-09-17T10:32:49.665434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"memory_usage = df_train_source.memory_usage().sum() / 1024**2\nprint(f'Training DataFrame Memory Usage = {memory_usage :.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:22.385120Z","iopub.execute_input":"2023-09-17T10:33:22.385521Z","iopub.status.idle":"2023-09-17T10:33:22.395361Z","shell.execute_reply.started":"2023-09-17T10:33:22.385493Z","shell.execute_reply":"2023-09-17T10:33:22.393286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data","metadata":{}},{"cell_type":"code","source":"df_train_source.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:22.398129Z","iopub.execute_input":"2023-09-17T10:33:22.398597Z","iopub.status.idle":"2023-09-17T10:33:22.434878Z","shell.execute_reply.started":"2023-09-17T10:33:22.398532Z","shell.execute_reply":"2023-09-17T10:33:22.433582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(17, 4))\nax = plt.gca()  # get current axes\n\nsns.barplot(x=df_train_source.isnull().sum().index, \n            y=df_train_source.isnull().sum().values / len(df_train_source) * 100,\n            ax=ax)\n\nplt.ylabel('Missing Values Percentage')\nax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter())\nplt.tick_params(axis='x')\nplt.tick_params(axis='y')\nplt.title('Missing Values')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:22.436883Z","iopub.execute_input":"2023-09-17T10:33:22.437241Z","iopub.status.idle":"2023-09-17T10:33:22.812616Z","shell.execute_reply.started":"2023-09-17T10:33:22.437212Z","shell.execute_reply":"2023-09-17T10:33:22.811455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replace NaN","metadata":{}},{"cell_type":"code","source":"def replace_nan(df: pd.DataFrame) -> pd.DataFrame:\n    df_ = df.copy()\n    df_['keyword'] = df_['keyword'].fillna('')\n    df_['location'] = df_['location'].fillna('')\n    return df_\n\ndf_train = replace_nan(df_train_source)\ndf_train","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:22.814654Z","iopub.execute_input":"2023-09-17T10:33:22.815403Z","iopub.status.idle":"2023-09-17T10:33:22.839692Z","shell.execute_reply.started":"2023-09-17T10:33:22.815361Z","shell.execute_reply":"2023-09-17T10:33:22.838418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Variable","metadata":{}},{"cell_type":"markdown","source":"### Target Labels Balance","metadata":{}},{"cell_type":"code","source":"ser = df_train['target']\nser.value_counts().index","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:22.842357Z","iopub.execute_input":"2023-09-17T10:33:22.842905Z","iopub.status.idle":"2023-09-17T10:33:22.851636Z","shell.execute_reply.started":"2023-09-17T10:33:22.842874Z","shell.execute_reply":"2023-09-17T10:33:22.850355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0 is negative\n# 1 is positive, i.e. real desaster\nprint(df_train['target'].value_counts())\n\nfig,(ax1,ax2)=plt.subplots(nrows=1,\n                           ncols=2,\n                           figsize=(15,5))\nax1.pie(x=df_train['target'].value_counts().values,\n       labels=df_train['target'].value_counts().index,\n       colors=sns.color_palette('pastel'),\n       autopct='%.0f%%')\n\nsns.countplot(x=df_train['target'],\n             ax=ax2,\n             palette=sns.color_palette('pastel'))\n\n# display absolute values\nabs_values = df_train['target'].value_counts().values\nax2.bar_label(container=ax2.containers[0], \n              labels=abs_values)\n\nplt.suptitle('Target Variable Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:22.852931Z","iopub.execute_input":"2023-09-17T10:33:22.853653Z","iopub.status.idle":"2023-09-17T10:33:23.197781Z","shell.execute_reply.started":"2023-09-17T10:33:22.853601Z","shell.execute_reply":"2023-09-17T10:33:23.196638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keyword and Location Features","metadata":{}},{"cell_type":"markdown","source":"## Distribution of Keywords","metadata":{}},{"cell_type":"code","source":"df_positive = df_train[df_train['target'] == 1] \ndf_negative = df_train[df_train['target'] == 0] \n\nkeywords_count_positive: list[tuple[str, int]] = list(df_positive['keyword'].value_counts().to_dict().items())[:25]\nkeywords_count_negative: list[tuple[str, int]] = list(df_negative['keyword'].value_counts().to_dict().items())[:25]","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:23.199393Z","iopub.execute_input":"2023-09-17T10:33:23.199861Z","iopub.status.idle":"2023-09-17T10:33:23.215710Z","shell.execute_reply.started":"2023-09-17T10:33:23.199820Z","shell.execute_reply":"2023-09-17T10:33:23.214505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_frequency(count_positive: list[tuple[str, int]],\n                   count_negative: list[tuple[str, int]],\n                   title = ''):\n\n    fig,(ax1,ax2)=plt.subplots(nrows=1,\n                               ncols=2,\n                               figsize=(15,5))\n    positive_plot = sns.barplot(x=[count for _, count in count_positive],\n                                y=[word for word, _ in count_positive],\n                                ax=ax1,\n                                color=\"#A7C7E7\")  # blue\n    positive_plot.set(title='Positive')\n    negative_plot = sns.barplot(x=[count for _, count in count_negative],\n                                y=[word for word, _ in count_negative],\n                                ax=ax2,\n                                color=\"#FF6961\")  # red\n    negative_plot.set(title='Negative')\n    \n    fig.suptitle(title)\n    plt.show()\n\nplot_frequency(keywords_count_positive, keywords_count_negative, 'Keywords')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:23.217252Z","iopub.execute_input":"2023-09-17T10:33:23.217626Z","iopub.status.idle":"2023-09-17T10:33:24.202250Z","shell.execute_reply.started":"2023-09-17T10:33:23.217597Z","shell.execute_reply":"2023-09-17T10:33:24.201123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Location","metadata":{}},{"cell_type":"code","source":"location_count_positive: list[tuple[str, int]] = list(df_positive['location'].value_counts().to_dict().items())[:25]\nlocation_count_negative: list[tuple[str, int]] = list(df_negative['location'].value_counts().to_dict().items())[:25]\n\nplot_frequency(location_count_positive, location_count_negative, 'Locations')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:24.204286Z","iopub.execute_input":"2023-09-17T10:33:24.205176Z","iopub.status.idle":"2023-09-17T10:33:25.250814Z","shell.execute_reply.started":"2023-09-17T10:33:24.205135Z","shell.execute_reply":"2023-09-17T10:33:25.249618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interim Conclusion\n- Keyword seems to be one of the criteria for data selection. That makes it a very dubious feature. We'll probably skip it.\n- The Location variable is probably too skewed and has too much missing data to gain any relevant insight. ","metadata":{}},{"cell_type":"markdown","source":"# Distribution of Text Length","metadata":{}},{"cell_type":"code","source":"# differ between negative and positive label\nser_positive = df_train[df_train['target'] == 1]['text'].str.lower()\nser_negative = df_train[df_train['target'] == 0]['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:25.252559Z","iopub.execute_input":"2023-09-17T10:33:25.253688Z","iopub.status.idle":"2023-09-17T10:33:25.269346Z","shell.execute_reply.started":"2023-09-17T10:33:25.253641Z","shell.execute_reply":"2023-09-17T10:33:25.268086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_count_by_label(ser_count_positive: pd.Series, \n                        ser_count_negative: pd.Series,\n                        xlabel = '',\n                        title = ''):\n\n    fig,(ax1,ax2)=plt.subplots(nrows=1,\n                               ncols=2,\n                               figsize=(15,5))\n    positive_histplot = sns.histplot(ser_count_positive,\n                 ax=ax1,\n                 bins=50, \n                 kde=True,\n                 color=\"#A7C7E7\")  # blue\n    positive_histplot.set(xlabel = xlabel,\n                          ylabel = \"Tweets\",\n                          title='Positive Label')\n    negative_histplot = sns.histplot(ser_count_negative,\n                                     ax=ax2,\n                                     bins=50, \n                                     kde=True,\n                                     color=\"#FF6961\")  # red\n    negative_histplot.set(xlabel = xlabel,\n                          ylabel = \"Tweets\",\n                          title='Negative Label')\n    fig.suptitle(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:25.270973Z","iopub.execute_input":"2023-09-17T10:33:25.271300Z","iopub.status.idle":"2023-09-17T10:33:25.284484Z","shell.execute_reply.started":"2023-09-17T10:33:25.271270Z","shell.execute_reply":"2023-09-17T10:33:25.282994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Characters","metadata":{}},{"cell_type":"code","source":"len_positive = ser_positive.apply(len)\nlen_negative = ser_negative.apply(len)\n\nplot_count_by_label(len_positive, len_negative, 'Number of Characters', 'Text Length (Characters) by Label')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:25.285949Z","iopub.execute_input":"2023-09-17T10:33:25.286401Z","iopub.status.idle":"2023-09-17T10:33:26.193400Z","shell.execute_reply.started":"2023-09-17T10:33:25.286360Z","shell.execute_reply":"2023-09-17T10:33:26.192228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Words","metadata":{}},{"cell_type":"code","source":"# a general approximation is sufficient for us here, so we don't use\n# a sophisticated tokenizer but use the very simple pandas split fn\n\nnumwords_positive = ser_positive.str.split().apply(len)\nnumwords_negative = ser_negative.str.split().apply(len)\n\nplot_count_by_label(numwords_positive, numwords_negative, 'Number of Words', 'Text Length (Words) by Label')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:26.194935Z","iopub.execute_input":"2023-09-17T10:33:26.195600Z","iopub.status.idle":"2023-09-17T10:33:27.052862Z","shell.execute_reply.started":"2023-09-17T10:33:26.195551Z","shell.execute_reply":"2023-09-17T10:33:27.051712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Punctuation Characters","metadata":{}},{"cell_type":"code","source":"# what counts as punctuation...\nstring.punctuation","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:27.058470Z","iopub.execute_input":"2023-09-17T10:33:27.058868Z","iopub.status.idle":"2023-09-17T10:33:27.066416Z","shell.execute_reply.started":"2023-09-17T10:33:27.058838Z","shell.execute_reply":"2023-09-17T10:33:27.065163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punct_positive = ser_positive.apply(lambda z: len([c for c in z if c in string.punctuation]))\npunct_negative = ser_negative.apply(lambda z:len([c for c in z if c in string.punctuation]))\n\nplot_count_by_label(punct_positive, punct_negative, 'Puncutation Characters', 'Number of Punctuation Characters by Label')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:27.068100Z","iopub.execute_input":"2023-09-17T10:33:27.068451Z","iopub.status.idle":"2023-09-17T10:33:28.036332Z","shell.execute_reply.started":"2023-09-17T10:33:27.068421Z","shell.execute_reply":"2023-09-17T10:33:28.035085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Percentage of Stopwords","metadata":{}},{"cell_type":"code","source":"# counting most frequently used phrases using nltk library \n# (e.g. \"doing\", \"them\", \"while\") \ncustom_stop_words = {'http', 'https', '-', 'I', 'The', 'A', '...', '&amp;'}\nstop_words = set(nltk.corpus.stopwords.words('english')) | custom_stop_words","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:28.037847Z","iopub.execute_input":"2023-09-17T10:33:28.038284Z","iopub.status.idle":"2023-09-17T10:33:28.049813Z","shell.execute_reply.started":"2023-09-17T10:33:28.038254Z","shell.execute_reply":"2023-09-17T10:33:28.048653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_stopwords_percentage(tweet: str) -> float:\n    tokenized = tweet.split()\n    return len([w for w in tokenized if w in stop_words]) / len(tokenized)\n                                \n\nstopw_positive = ser_positive.apply(compute_stopwords_percentage)\nstopw_negative = ser_negative.apply(compute_stopwords_percentage)\n\nplot_count_by_label(stopw_positive, stopw_negative, 'Percentage of Stopwords', 'Percentage of Stopwords by Label')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:28.051067Z","iopub.execute_input":"2023-09-17T10:33:28.052373Z","iopub.status.idle":"2023-09-17T10:33:28.979151Z","shell.execute_reply.started":"2023-09-17T10:33:28.052333Z","shell.execute_reply":"2023-09-17T10:33:28.978089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of URLs","metadata":{}},{"cell_type":"code","source":"urls_positive = ser_positive.apply(lambda words: len([w for w in words.split() if 'http' in w or 'https' in w]))\nurls_negative = ser_negative.apply(lambda words: len([w for w in words.split() if 'http' in w or 'https' in w]))\n\nplot_count_by_label(urls_positive, urls_negative, 'Number of URLs', 'Number of URLs by Label')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:28.980432Z","iopub.execute_input":"2023-09-17T10:33:28.981484Z","iopub.status.idle":"2023-09-17T10:33:29.884755Z","shell.execute_reply.started":"2023-09-17T10:33:28.981445Z","shell.execute_reply":"2023-09-17T10:33:29.883460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Hashtags (#)","metadata":{}},{"cell_type":"code","source":"hashtags_positive = ser_positive.apply(lambda t: len([c for c in t if c == '#']))\nhashtags_negative = ser_negative.apply(lambda t: len([c for c in t if c == '#']))\n\nplot_count_by_label(hashtags_positive, hashtags_negative, 'Number of Hashtags',  'Number of Hashtags by Label')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:29.886133Z","iopub.execute_input":"2023-09-17T10:33:29.886500Z","iopub.status.idle":"2023-09-17T10:33:30.779524Z","shell.execute_reply.started":"2023-09-17T10:33:29.886471Z","shell.execute_reply":"2023-09-17T10:33:30.775334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# n-gram Analysis","metadata":{}},{"cell_type":"markdown","source":"## Most frequent Words","metadata":{}},{"cell_type":"code","source":"def get_most_frequent_words(ser: pd.Series):\n    # create one list of words for the whole series\n    corpus = [word.lower() for tokenized in ser.str.split() for word in tokenized if word not in stop_words]\n        \n    # create a list of words with their respective count\n    counter = Counter(corpus)\n    most_common: list[tuple[str, int]] = counter.most_common(30)\n    return most_common\n\nmost_common_positive = get_most_frequent_words(ser_positive)\nprint(most_common_positive[:5])\n\nmost_common_negative = get_most_frequent_words(ser_negative)\nprint(most_common_negative[:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:30.781479Z","iopub.execute_input":"2023-09-17T10:33:30.782037Z","iopub.status.idle":"2023-09-17T10:33:31.083769Z","shell.execute_reply.started":"2023-09-17T10:33:30.781989Z","shell.execute_reply":"2023-09-17T10:33:31.082475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_frequency(most_common_positive, most_common_negative, 'Most frequent Words')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:31.085596Z","iopub.execute_input":"2023-09-17T10:33:31.086051Z","iopub.status.idle":"2023-09-17T10:33:32.039045Z","shell.execute_reply.started":"2023-09-17T10:33:31.086010Z","shell.execute_reply":"2023-09-17T10:33:32.037906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud\nLooks nice, but basically useless. Still, we plot some WordClouds...","metadata":{}},{"cell_type":"code","source":"# wordcloud.WordCloud Visualization\ndef display_cloud(ser_positive: pd.Series, ser_negative: pd.Series):\n    \n    fig,(ax1,ax2)=plt.subplots(nrows=1,\n                               ncols=2,\n                               figsize=(15,5))\n    \n    wc_positive = WordCloud(stopwords=stop_words,\n                   background_color=\"white\",\n                   random_state=my_seed,\n                  )\n    wc_positive.generate(' '.join(ser_positive))\n    ax1.imshow(wc_positive,\n               interpolation=\"bilinear\")\n    ax1.axis('off')\n    ax1.set(title='Positive Label')\n    \n    wc_negative = WordCloud(stopwords=stop_words,\n                   background_color=\"white\",\n                   random_state=my_seed\n                  )\n    wc_negative.generate(' '.join(ser_negative))\n    ax2.imshow(wc_negative,\n               interpolation=\"bilinear\")\n    ax2.axis('off')\n    ax2.set(title='Negative Label')\n\n    plt.show()\n\ndisplay_cloud(df_train[df_train['target'] == 1]['text'],\n              df_train[df_train['target'] == 0]['text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:32.040485Z","iopub.execute_input":"2023-09-17T10:33:32.041463Z","iopub.status.idle":"2023-09-17T10:33:33.995406Z","shell.execute_reply.started":"2023-09-17T10:33:32.041423Z","shell.execute_reply":"2023-09-17T10:33:33.994402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-gram","metadata":{}},{"cell_type":"code","source":"def gram_analysis(review: str,\n                  n_gram: int) -> list[str]:  # returns a list of strings, each containing n_gram word tokens \n                                              # (stopwords ignored)\n    tokens=[t for t in review.lower().split(\" \") if t!=\"\" if t not in stop_words]\n    ngrams=zip(*[tokens[i:] for i in range(n_gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n# Example:\n# gram_analysis(review='Why can\\'t a movie be rated a zero? Or even a negative number? Some movie rated 1 is \n# so bad they\\'re fun to watch.', n_gram=2)\n# --> ['can't movie', 'movie rated', 'rated zero?', 'zero? even', 'even negative', 'negative number?', \n#      'number? movie', 'movie rated', 'rated 1', '1 bad', \"bad they're\", \"they're fun\", 'fun watch.']","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:33.996614Z","iopub.execute_input":"2023-09-17T10:33:33.997542Z","iopub.status.idle":"2023-09-17T10:33:34.004287Z","shell.execute_reply.started":"2023-09-17T10:33:33.997506Z","shell.execute_reply":"2023-09-17T10:33:34.003004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create frequency grams for analysis\ndef get_frequency_dict(ser_reviews: pd.Series,\n                n_gram: int) -> dict[str, int]:\n    frequency_dict = defaultdict(int)\n    for sentence in ser_reviews:\n        for tokens in gram_analysis(sentence, n_gram):\n            frequency_dict[tokens]+=1\n    return dict(frequency_dict)\n\n# Example:\n# ser = pd.Series(['Why can\\'t a movie be rated a zero? Or even a negative number?',\n#                  'Some movie rated 1 is so bad they\\'re fun to watch.'])\n# create_dict(ser, n_gram=2)\n# --> defaultdict(<class 'int'>, {'can't movie': 1, 'movie rated': 2, 'rated zero?': 1, 'zero? even': 1, \n#                                'even negative': 1, 'negative number?': 1, 'rated 1': 1, '1 bad': 1, \"bad they're\": 1, \"they're fun\": 1, 'fun watch.': 1})","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:34.005690Z","iopub.execute_input":"2023-09-17T10:33:34.006016Z","iopub.status.idle":"2023-09-17T10:33:34.021608Z","shell.execute_reply.started":"2023-09-17T10:33:34.005985Z","shell.execute_reply":"2023-09-17T10:33:34.020441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigram_frequency_positive = get_frequency_dict(ser_positive,\n                                          n_gram=2)\nbigram_frequency_negative = get_frequency_dict(ser_negative,\n                                          n_gram=2)\n\nprint(f'Found a total of {len(bigram_frequency_positive) :n} distinct bigrams in positive reviews.')\nprint(f'Found a total of {len(bigram_frequency_negative) :n} distinct bigrams in negative reviews.')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:33:34.023041Z","iopub.execute_input":"2023-09-17T10:33:34.023448Z","iopub.status.idle":"2023-09-17T10:33:34.143853Z","shell.execute_reply.started":"2023-09-17T10:33:34.023418Z","shell.execute_reply":"2023-09-17T10:33:34.142638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def barplot_on_axes(frequency_dict: dict[str, int], \n                    title: str,\n                    ax: Axes,\n                    color: str):\n    \n    # convert dict (gram-as-string to count) to list of tuples (gram, count) in descending order by count\n    sorted_n_grams = sorted(frequency_dict.items(),\n                            key=lambda z:z[1],\n                            reverse=True)\n    sorted_n_grams = sorted_n_grams[:25]\n    \n    barplot = sns.barplot(x=[b[0] for b in sorted_n_grams], \n                y=[b[1] for b in sorted_n_grams], \n                ax=ax,\n                color=color)  # blue\n    \n    ax.tick_params(axis='x', \n                   rotation=90)\n    barplot.set(xlabel='n-grams',\n                ylabel=\"Tweets\",\n                title=title)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:36:52.987349Z","iopub.execute_input":"2023-09-17T10:36:52.987823Z","iopub.status.idle":"2023-09-17T10:36:52.997076Z","shell.execute_reply.started":"2023-09-17T10:36:52.987789Z","shell.execute_reply":"2023-09-17T10:36:52.995556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(nrows=1,\n                           ncols=2,\n                           figsize=(15,5))\nbarplot_on_axes(bigram_frequency_positive, 'Positive Label', ax1, color=\"#A7C7E7\")  # blue\nbarplot_on_axes(bigram_frequency_negative, 'Negative Label', ax2, color=\"#FF6961\")  # red\nfig.suptitle('Most frequent 2-grams')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:36:54.036199Z","iopub.execute_input":"2023-09-17T10:36:54.036608Z","iopub.status.idle":"2023-09-17T10:36:55.075616Z","shell.execute_reply.started":"2023-09-17T10:36:54.036564Z","shell.execute_reply":"2023-09-17T10:36:55.074350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3-gram","metadata":{}},{"cell_type":"code","source":"trigram_frequency_positive = get_frequency_dict(ser_positive,\n                                          n_gram=3)\ntrigram_frequency_negative = get_frequency_dict(ser_negative,\n                                          n_gram=3)\n\nfig,(ax1,ax2)=plt.subplots(nrows=1,\n                           ncols=2,\n                           figsize=(15,5))\nbarplot_on_axes(trigram_frequency_positive, 'Positive Label', ax1, color=\"#A7C7E7\")  # blue\nbarplot_on_axes(trigram_frequency_negative, 'Negative Label', ax2, color=\"#FF6961\")  # red\nfig.suptitle('Most frequent 3-grams')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:37:11.946339Z","iopub.execute_input":"2023-09-17T10:37:11.946820Z","iopub.status.idle":"2023-09-17T10:37:13.442973Z","shell.execute_reply.started":"2023-09-17T10:37:11.946785Z","shell.execute_reply":"2023-09-17T10:37:13.441758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}