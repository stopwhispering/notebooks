{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Disaster Tweets - Classification #2 - Pretrained Embeddings in Linear NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel includes codes and ideas from...\n",
    "- https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca\n",
    "\n",
    "Others:\n",
    "- Compatible with Google Colab and Kaggle as runtime\n",
    "- CUDA support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:44:47.419336Z",
     "iopub.status.busy": "2023-09-18T06:44:47.418848Z",
     "iopub.status.idle": "2023-09-18T06:44:50.821526Z",
     "shell.execute_reply": "2023-09-18T06:44:50.820466Z",
     "shell.execute_reply.started": "2023-09-18T06:44:47.419299Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {DEVICE}')\n",
    "\n",
    "# running in google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    BASE_PATH = './drive/MyDrive/Colab/data/'\n",
    "    BASE_PATH_PRETRAINED = './drive/MyDrive/Colab/pretrained/'\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    !pip install tokenizers\n",
    "\n",
    "# running interactively in kaggle\n",
    "elif get_ipython().config.IPKernelApp.connection_file.startswith('/root/.local/share'):\n",
    "    BASE_PATH = '/kaggle/input/'\n",
    "    BASE_PATH_PRETRAINED = '/kaggle/input/'\n",
    "    \n",
    "# running as background job in kaggle\n",
    "elif 'SHLVL' in os.environ:\n",
    "    BASE_PATH = '/kaggle/input/'\n",
    "    BASE_PATH_PRETRAINED = '/kaggle/input/'\n",
    "\n",
    "else:\n",
    "    BASE_PATH = '../data/'\n",
    "    BASE_PATH_PRETRAINED = '../pretrained/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:44:53.051361Z",
     "iopub.status.busy": "2023-09-18T06:44:53.050837Z",
     "iopub.status.idle": "2023-09-18T06:45:28.913941Z",
     "shell.execute_reply": "2023-09-18T06:45:28.912973Z",
     "shell.execute_reply.started": "2023-09-18T06:44:53.051303Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, locale='')  # for thousands separator via ... print(f'{value:n}')\"\n",
    "import re\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from typing import Callable\n",
    "import json\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import matplotlib.ticker\n",
    "from matplotlib.axes._axes import Axes\n",
    "import nltk\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import BertModel\n",
    "import gensim\n",
    "\n",
    "my_seed = 42\n",
    "random.seed(my_seed)\n",
    "torch.manual_seed(my_seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:45:28.916393Z",
     "iopub.status.busy": "2023-09-18T06:45:28.915973Z",
     "iopub.status.idle": "2023-09-18T06:45:28.961592Z",
     "shell.execute_reply": "2023-09-18T06:45:28.960703Z",
     "shell.execute_reply.started": "2023-09-18T06:45:28.916278Z"
    }
   },
   "outputs": [],
   "source": [
    "df_source = pd.read_csv(BASE_PATH + 'nlp-getting-started/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:45:28.963898Z",
     "iopub.status.busy": "2023-09-18T06:45:28.963220Z",
     "iopub.status.idle": "2023-09-18T06:45:29.003162Z",
     "shell.execute_reply": "2023-09-18T06:45:29.002232Z",
     "shell.execute_reply.started": "2023-09-18T06:45:28.963865Z"
    }
   },
   "outputs": [],
   "source": [
    "df_randomized = df_source.sample(frac=1)\n",
    "NUM_VAL = int(len(df_randomized) * 0.15)\n",
    "\n",
    "df_train_source = df_randomized[:-NUM_VAL]\n",
    "df_val_source = df_randomized[-NUM_VAL:]\n",
    "\n",
    "def replace_nan(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_ = df.copy()\n",
    "    df_['keyword'] = df_['keyword'].fillna('')\n",
    "    df_['location'] = df_['location'].fillna('')\n",
    "    return df_\n",
    "\n",
    "df_train = replace_nan(df_train_source)\n",
    "df_val = replace_nan(df_val_source)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproces\n",
    "For more details and explanation, see Classification #1 Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:45:29.006178Z",
     "iopub.status.busy": "2023-09-18T06:45:29.005844Z",
     "iopub.status.idle": "2023-09-18T06:45:29.171989Z",
     "shell.execute_reply": "2023-09-18T06:45:29.171036Z",
     "shell.execute_reply.started": "2023-09-18T06:45:29.006147Z"
    }
   },
   "outputs": [],
   "source": [
    "normalizer = normalizers.Sequence([\n",
    "    NFD(),   # NFD unicode normalization\n",
    "    Lowercase(),\n",
    "    StripAccents()  #\n",
    "])\n",
    "\n",
    "REGEX_HASHTAG_BEFORE = r'(?<!\\S)#(\\S+)'\n",
    "REGEX_HASHTAG_AFTER = r'\\1'\n",
    "def strip_hashtags(tweet: str) -> str:\n",
    "    return re.sub(REGEX_HASHTAG_BEFORE, REGEX_HASHTAG_AFTER, tweet)\n",
    "\n",
    "punct = re.compile(r'[^\\w\\s]')\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    return punct.sub(r'', text)\n",
    "\n",
    "dl_url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\n",
    "british_to_american_map = requests.get(dl_url).json()\n",
    "def americanize(text: str):\n",
    "    tokenized = nltk.tokenize.word_tokenize(text)\n",
    "    americanized = [british_to_american_map[w] if w in british_to_american_map\n",
    "                    else w for w in tokenized]\n",
    "    return ' '.join(americanized)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "def remove_stopwords(text: str):\n",
    "    tokenized = nltk.tokenize.word_tokenize(text)\n",
    "    without_stopwords = [word for word in tokenized if word.lower() not in stop_words]\n",
    "    return ' '.join(without_stopwords)\n",
    "\n",
    "spelling_dict = {\n",
    "    'didnt': \"didn't\",\n",
    "    'doesnt': \"doesn't\",\n",
    "    'isnt': \"isn't\",\n",
    "    'aint': \"ain't\",\n",
    "    'wasnt': \"wasn't\",\n",
    "    'shouldnt': \"shoudn't\",\n",
    "    'im': \"i'm\",\n",
    "}\n",
    "def rectify_spelling(text: str):\n",
    "    tokenized = nltk.tokenize.word_tokenize(text)\n",
    "    corrected = [spelling_dict.get(w, w) for w in tokenized]\n",
    "    return ' '.join(corrected)\n",
    "    \n",
    "with open(BASE_PATH + \"contractions/english_contractions.json\", \"r\") as f:\n",
    "    contraction_mapping = json.load(f)\n",
    "def replace_contraction(tweet: str):\n",
    "    for contraction, full_form in contraction_mapping.items():\n",
    "        tweet = re.sub(contraction, full_form, tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:45:29.174025Z",
     "iopub.status.busy": "2023-09-18T06:45:29.173472Z",
     "iopub.status.idle": "2023-09-18T06:45:35.585561Z",
     "shell.execute_reply": "2023-09-18T06:45:35.584477Z",
     "shell.execute_reply.started": "2023-09-18T06:45:29.173992Z"
    }
   },
   "outputs": [],
   "source": [
    "ser_train = (df_train['text']\n",
    "             .apply(normalizer.normalize_str)\n",
    "             .apply(strip_hashtags)\n",
    "             .apply(remove_punctuations)\n",
    "             .apply(americanize)\n",
    "             .apply(remove_stopwords)\n",
    "             .apply(rectify_spelling)\n",
    "             .apply(replace_contraction)\n",
    "            )\n",
    "\n",
    "ser_val = (df_val['text']\n",
    "             .apply(normalizer.normalize_str)\n",
    "             .apply(strip_hashtags)\n",
    "             .apply(remove_punctuations)\n",
    "             .apply(americanize)\n",
    "             .apply(remove_stopwords)\n",
    "             .apply(rectify_spelling)\n",
    "             .apply(replace_contraction)\n",
    "            )\n",
    "\n",
    "ser_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec pretrained Embeddings\n",
    "For mode details and embedding coverage, see Classification #1 Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:45:35.587643Z",
     "iopub.status.busy": "2023-09-18T06:45:35.587270Z",
     "iopub.status.idle": "2023-09-18T06:46:47.373319Z",
     "shell.execute_reply": "2023-09-18T06:46:47.372355Z",
     "shell.execute_reply.started": "2023-09-18T06:45:35.587611Z"
    }
   },
   "outputs": [],
   "source": [
    "path = BASE_PATH_PRETRAINED + 'googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(path, \n",
    "                                                     binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceae6139-1392-4820-b303-5719e973e29d"
   },
   "source": [
    "### Create Vocabulary and Custom Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:46:47.375171Z",
     "iopub.status.busy": "2023-09-18T06:46:47.374789Z",
     "iopub.status.idle": "2023-09-18T06:46:48.579871Z",
     "shell.execute_reply": "2023-09-18T06:46:48.578738Z",
     "shell.execute_reply.started": "2023-09-18T06:46:47.375136Z"
    },
    "id": "5a579f15-9e3b-4e65-86fb-050de116b0d9",
    "outputId": "35bcf018-7a51-478c-e327-179323107020"
   },
   "outputs": [],
   "source": [
    "flat_words = [word for text in ser_train for word in nltk.tokenize.word_tokenize(text)]\n",
    "distinct_words = set(flat_words)\n",
    "print(f'{len(distinct_words)} distinct words.')\n",
    "\n",
    "words_with_embeddings = [w for w in distinct_words if w in wv.key_to_index]\n",
    "words_without_embeddings = [w for w in distinct_words if w not in wv.key_to_index]\n",
    "print(f'{len(words_with_embeddings)} words with pretrained word vectors.')\n",
    "print(f'{len(words_without_embeddings)} words without pretrained word vectors. We will ignore them.')\n",
    "\n",
    "token_to_index = {token: index for index, token in enumerate(words_with_embeddings)}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "\n",
    "# wrapper for token-to-index mapping\n",
    "vocab = torchtext.vocab.vocab(token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:46:48.581720Z",
     "iopub.status.busy": "2023-09-18T06:46:48.581280Z",
     "iopub.status.idle": "2023-09-18T06:46:48.832239Z",
     "shell.execute_reply": "2023-09-18T06:46:48.831242Z",
     "shell.execute_reply.started": "2023-09-18T06:46:48.581686Z"
    },
    "id": "ce7b0876-d1fb-498f-bea3-5ae5aa9d1ee3",
    "outputId": "593afc16-8ad6-408d-98fe-5d637ca4acd0"
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<pad>'\n",
    "\n",
    "# Create initiual embeddings with all-zeros in 300 dimensions (like pretrained embeddings)\n",
    "embeddings = torch.zeros(len(token_to_index), wv.vectors.shape[1])\n",
    "print(embeddings.shape)\n",
    "\n",
    "# we use the known words' embeddings in our model\n",
    "indices_with_embeddings = [token_to_index[w] for w in words_with_embeddings]\n",
    "\n",
    "# map from \"new\" to \"old\" index (i.e. pretrained index)\n",
    "index_to_pretrained_index = {index: wv.key_to_index[index_to_token[index]] for index in indices_with_embeddings}\n",
    "\n",
    "for index, pretrained_index in index_to_pretrained_index.items():\n",
    "    embeddings[index] = torch.Tensor(wv.vectors[pretrained_index])  # ndarray to tensor\n",
    "\n",
    "# add the padding token (we'll need it later)\n",
    "if PAD_TOKEN not in token_to_index:\n",
    "    PAD_TOKEN_IDX = len(embeddings)\n",
    "    token_to_index[PAD_TOKEN] = PAD_TOKEN_IDX\n",
    "    index_to_token[PAD_TOKEN_IDX] = PAD_TOKEN\n",
    "    embeddings = torch.cat([embeddings, \n",
    "                        torch.zeros(1, wv.vectors.shape[1])])\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:46:48.834294Z",
     "iopub.status.busy": "2023-09-18T06:46:48.833893Z",
     "iopub.status.idle": "2023-09-18T06:46:48.838619Z",
     "shell.execute_reply": "2023-09-18T06:46:48.837665Z",
     "shell.execute_reply.started": "2023-09-18T06:46:48.834260Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:46:48.842757Z",
     "iopub.status.busy": "2023-09-18T06:46:48.842149Z",
     "iopub.status.idle": "2023-09-18T06:46:50.419633Z",
     "shell.execute_reply": "2023-09-18T06:46:50.418650Z",
     "shell.execute_reply.started": "2023-09-18T06:46:48.842724Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_if_in_vocab(text: str) -> list[str]:\n",
    "    tokens = [w for w in nltk.tokenize.word_tokenize(text) if w in vocab]\n",
    "    if len(tokens) < MAX_WORDS:\n",
    "        tokens = tokens + [PAD_TOKEN] * (MAX_WORDS - len(tokens))\n",
    "    elif len(tokens) > MAX_WORDS:\n",
    "        tokens = tokens[:50]\n",
    "    return tokens\n",
    "\n",
    "tokenized_train = ser_train.apply(tokenize_if_in_vocab)\n",
    "print(f'tokenized_train is a {type(tokenized_train)} of shape {tokenized_train.shape}')\n",
    "\n",
    "tokenized_val = ser_val.apply(tokenize_if_in_vocab)\n",
    "print(f'tokenized_val is a {type(tokenized_val)} of shape {tokenized_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:46:50.421882Z",
     "iopub.status.busy": "2023-09-18T06:46:50.420937Z",
     "iopub.status.idle": "2023-09-18T06:46:50.579290Z",
     "shell.execute_reply": "2023-09-18T06:46:50.578169Z",
     "shell.execute_reply.started": "2023-09-18T06:46:50.421847Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenized_train is a pd.Series with each element being a list of size MAX_WORDS containing words (or PAD_TOKEN)\n",
    "# we need to convert that to a np.Array of size [n, MAX_WORDS]\n",
    "\n",
    "def convert_token_to_index(tokens: list[str]) -> torch.Tensor:\n",
    "    context_indices: list[int] = []\n",
    "    for token in tokens:\n",
    "        index = token_to_index[token]\n",
    "        context_indices.append(index)\n",
    "\n",
    "    return context_indices\n",
    "    \n",
    "indexed_train = tokenized_train.apply(convert_token_to_index) \n",
    "indexed_val = tokenized_val.apply(convert_token_to_index) \n",
    "\n",
    "x_train_arr = np.stack(indexed_train.values)  # array of size (5330, 50)\n",
    "x_val_arr = np.stack(indexed_val.values)  # array of size (2283, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:46:50.581103Z",
     "iopub.status.busy": "2023-09-18T06:46:50.580737Z",
     "iopub.status.idle": "2023-09-18T06:46:50.586360Z",
     "shell.execute_reply": "2023-09-18T06:46:50.585433Z",
     "shell.execute_reply.started": "2023-09-18T06:46:50.581063Z"
    }
   },
   "outputs": [],
   "source": [
    "ser_y_train = df_train_source['target']\n",
    "ser_y_val = df_val_source['target']\n",
    "\n",
    "assert x_train_arr.shape[0] == ser_y_train.shape[0]\n",
    "assert x_val_arr.shape[0] == ser_y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:47:19.251504Z",
     "iopub.status.busy": "2023-09-18T06:47:19.251107Z",
     "iopub.status.idle": "2023-09-18T06:47:19.265890Z",
     "shell.execute_reply": "2023-09-18T06:47:19.262576Z",
     "shell.execute_reply.started": "2023-09-18T06:47:19.251475Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train_arr).to(DEVICE)  # [5330, 50], torch.int32\n",
    "x_val = torch.tensor(x_val_arr).to(DEVICE)   # [2283, 300], torch.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:47:31.456852Z",
     "iopub.status.busy": "2023-09-18T06:47:31.456498Z",
     "iopub.status.idle": "2023-09-18T06:47:31.462802Z",
     "shell.execute_reply": "2023-09-18T06:47:31.461696Z",
     "shell.execute_reply.started": "2023-09-18T06:47:31.456816Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = torch.tensor(ser_y_train.values).to(DEVICE)  # [5330], torch.int64\n",
    "y_val = torch.tensor(ser_y_val.values).to(DEVICE)  # [2283], torch.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:47:35.252509Z",
     "iopub.status.busy": "2023-09-18T06:47:35.252125Z",
     "iopub.status.idle": "2023-09-18T06:47:35.258408Z",
     "shell.execute_reply": "2023-09-18T06:47:35.256362Z",
     "shell.execute_reply.started": "2023-09-18T06:47:35.252477Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:47:36.134214Z",
     "iopub.status.busy": "2023-09-18T06:47:36.133151Z",
     "iopub.status.idle": "2023-09-18T06:47:36.141809Z",
     "shell.execute_reply": "2023-09-18T06:47:36.140736Z",
     "shell.execute_reply.started": "2023-09-18T06:47:36.134170Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(x_train, \n",
    "                                               y_train)\n",
    "\n",
    "# we don't need a DataLoader for validation data; we're going to predict\n",
    "# with validation data as a whole, without batches\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:53:51.494766Z",
     "iopub.status.busy": "2023-09-18T06:53:51.494411Z",
     "iopub.status.idle": "2023-09-18T06:53:51.504469Z",
     "shell.execute_reply": "2023-09-18T06:53:51.501585Z",
     "shell.execute_reply.started": "2023-09-18T06:53:51.494737Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 300\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, token_embedding: torch.Tensor):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.token_embedding = nn.Embedding.from_pretrained(token_embedding,\n",
    "                                                            freeze=False)  # True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(EMBEDDINGS_DIM*MAX_WORDS, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:53:51.822054Z",
     "iopub.status.busy": "2023-09-18T06:53:51.820870Z",
     "iopub.status.idle": "2023-09-18T06:53:51.855269Z",
     "shell.execute_reply": "2023-09-18T06:53:51.854364Z",
     "shell.execute_reply.started": "2023-09-18T06:53:51.822014Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "classifier = Classifier(token_embedding=embeddings.to(DEVICE)).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn =  nn.CrossEntropyLoss()  # nn.BCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:53:51.972095Z",
     "iopub.status.busy": "2023-09-18T06:53:51.971807Z",
     "iopub.status.idle": "2023-09-18T06:53:51.978862Z",
     "shell.execute_reply": "2023-09-18T06:53:51.977618Z",
     "shell.execute_reply.started": "2023-09-18T06:53:51.972070Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(classifier: Classifier, \n",
    "                    loss_fn: Callable,\n",
    "                    x: torch.Tensor, \n",
    "                    y: torch.Tensor\n",
    "                   )->tuple[float, float, float]:\n",
    "    \n",
    "        y_pred_logits = classifier(x)\n",
    "        loss = loss_fn(y_pred_logits, y).item()\n",
    "    \n",
    "        y_pred = y_pred_logits.argmax(dim=1)\n",
    "        correct = (y_pred == y).type(torch.FloatTensor)\n",
    "        accuracy = correct.mean().item()\n",
    "\n",
    "        f1_score = sklearn.metrics.f1_score(y_true=y.cpu(), \n",
    "                                    y_pred=y_pred.cpu())\n",
    "        \n",
    "        return loss, accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:53:52.137642Z",
     "iopub.status.busy": "2023-09-18T06:53:52.137300Z",
     "iopub.status.idle": "2023-09-18T06:53:54.595750Z",
     "shell.execute_reply": "2023-09-18T06:53:54.594691Z",
     "shell.execute_reply.started": "2023-09-18T06:53:52.137616Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "metrics = pd.DataFrame(columns=['loss_train', 'accuracy_train', 'f1_train', \n",
    "                                'loss_val', 'accuracy_val', 'f1_val'],\n",
    "                       index=range(NUM_EPOCHS))\n",
    "\n",
    "classifier.train()\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    # x_train_batch: [batch_size, 50], torch.int32\n",
    "    # y_train_batch: [batch_size], torch.int64\n",
    "    for x_train_batch, y_train_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_batch = classifier(x_train_batch)  # [batch_size, 2] of dtype torch.float32\n",
    "        \n",
    "        loss = loss_fn(y_pred_batch, y_train_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_train, accuracy_train, f1_score_train = compute_metrics(classifier, loss_fn, x_train, y_train)\n",
    "        loss_val, accuracy_val, f1_score_val = compute_metrics(classifier, loss_fn, x_val, y_val)\n",
    "        metrics.iloc[epoch] = [loss_train, accuracy_train, f1_score_train,\n",
    "                               loss_val, accuracy_val, f1_score_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:53:55.927387Z",
     "iopub.status.busy": "2023-09-18T06:53:55.926713Z",
     "iopub.status.idle": "2023-09-18T06:53:55.942014Z",
     "shell.execute_reply": "2023-09-18T06:53:55.940855Z",
     "shell.execute_reply.started": "2023-09-18T06:53:55.927353Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T06:53:56.083833Z",
     "iopub.status.busy": "2023-09-18T06:53:56.083165Z",
     "iopub.status.idle": "2023-09-18T06:53:56.948046Z",
     "shell.execute_reply": "2023-09-18T06:53:56.947200Z",
     "shell.execute_reply.started": "2023-09-18T06:53:56.083802Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = range(NUM_EPOCHS)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, _)) = plt.subplots(nrows=2,\n",
    "                                       ncols=2,\n",
    "                                       figsize=(15,5),\n",
    "                                          sharex=True)\n",
    "\n",
    "# Plot and label the training and val loss values\n",
    "ax1.plot(epochs, metrics['loss_train'], label='Training Loss')\n",
    "ax1.plot(epochs, metrics['loss_val'], label='val Loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "# ... Accuracy\n",
    "ax2.plot(epochs, metrics['accuracy_train'], label='Training Accuracy')\n",
    "ax2.plot(epochs, metrics['accuracy_val'], label='val Accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "# ... F1-Score\n",
    "ax3.plot(epochs, metrics['f1_train'], label='Training F1-Score')\n",
    "ax3.plot(epochs, metrics['f1_val'], label='val F1-Score')\n",
    "ax3.set_ylabel('F1-Score')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_xticks(np.arange(0, \n",
    "                         NUM_EPOCHS))\n",
    "\n",
    "plt.suptitle('Training and Validation Metrics')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0, \n",
    "                     NUM_EPOCHS))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:24:54.667260Z",
     "iopub.status.busy": "2023-09-18T07:24:54.666221Z",
     "iopub.status.idle": "2023-09-18T07:24:54.697148Z",
     "shell.execute_reply": "2023-09-18T07:24:54.695712Z",
     "shell.execute_reply.started": "2023-09-18T07:24:54.667216Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_source = pd.read_csv(BASE_PATH + 'nlp-getting-started/test.csv')\n",
    "df_test = replace_nan(df_test_source)\n",
    "df_test  # no target col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:25:29.202205Z",
     "iopub.status.busy": "2023-09-18T07:25:29.201833Z",
     "iopub.status.idle": "2023-09-18T07:25:31.948581Z",
     "shell.execute_reply": "2023-09-18T07:25:31.947528Z",
     "shell.execute_reply.started": "2023-09-18T07:25:29.202176Z"
    }
   },
   "outputs": [],
   "source": [
    "ser_test = (df_test['text']\n",
    "             .apply(normalizer.normalize_str)\n",
    "             .apply(strip_hashtags)\n",
    "             .apply(remove_punctuations)\n",
    "             .apply(americanize)\n",
    "             .apply(remove_stopwords)\n",
    "             .apply(rectify_spelling)\n",
    "             .apply(replace_contraction)\n",
    "            )\n",
    "\n",
    "ser_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:27:08.094119Z",
     "iopub.status.busy": "2023-09-18T07:27:08.093735Z",
     "iopub.status.idle": "2023-09-18T07:27:08.825934Z",
     "shell.execute_reply": "2023-09-18T07:27:08.824961Z",
     "shell.execute_reply.started": "2023-09-18T07:27:08.094089Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_test = ser_test.apply(tokenize_if_in_vocab)\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:28:13.894952Z",
     "iopub.status.busy": "2023-09-18T07:28:13.894214Z",
     "iopub.status.idle": "2023-09-18T07:28:13.946349Z",
     "shell.execute_reply": "2023-09-18T07:28:13.945344Z",
     "shell.execute_reply.started": "2023-09-18T07:28:13.894917Z"
    }
   },
   "outputs": [],
   "source": [
    "indexed_test = tokenized_test.apply(convert_token_to_index)\n",
    "indexed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:28:53.534371Z",
     "iopub.status.busy": "2023-09-18T07:28:53.533977Z",
     "iopub.status.idle": "2023-09-18T07:28:53.572598Z",
     "shell.execute_reply": "2023-09-18T07:28:53.571454Z",
     "shell.execute_reply.started": "2023-09-18T07:28:53.534333Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test_arr = np.stack(indexed_test.values)  # array of size (3263, 50)\n",
    "x_test_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:29:17.880106Z",
     "iopub.status.busy": "2023-09-18T07:29:17.879425Z",
     "iopub.status.idle": "2023-09-18T07:29:17.891133Z",
     "shell.execute_reply": "2023-09-18T07:29:17.889633Z",
     "shell.execute_reply.started": "2023-09-18T07:29:17.880072Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test = torch.tensor(x_test_arr).to(DEVICE)  # [3263, 50], torch.int64\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:32:31.226945Z",
     "iopub.status.busy": "2023-09-18T07:32:31.226482Z",
     "iopub.status.idle": "2023-09-18T07:32:31.259231Z",
     "shell.execute_reply": "2023-09-18T07:32:31.258171Z",
     "shell.execute_reply.started": "2023-09-18T07:32:31.226892Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_logits = classifier(x_test)\n",
    "    y_pred = y_pred_logits.argmax(dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:35:26.340235Z",
     "iopub.status.busy": "2023-09-18T07:35:26.339865Z",
     "iopub.status.idle": "2023-09-18T07:35:26.350091Z",
     "shell.execute_reply": "2023-09-18T07:35:26.348959Z",
     "shell.execute_reply.started": "2023-09-18T07:35:26.340204Z"
    }
   },
   "outputs": [],
   "source": [
    "ser_pred = pd.Series(y_pred.cpu().numpy())\n",
    "ser_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:35:15.339734Z",
     "iopub.status.busy": "2023-09-18T07:35:15.339232Z",
     "iopub.status.idle": "2023-09-18T07:35:15.352861Z",
     "shell.execute_reply": "2023-09-18T07:35:15.351785Z",
     "shell.execute_reply.started": "2023-09-18T07:35:15.339693Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'id': df_test['id'],\n",
    "                       'target': ser_pred})\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:35:45.948260Z",
     "iopub.status.busy": "2023-09-18T07:35:45.947912Z",
     "iopub.status.idle": "2023-09-18T07:35:45.956336Z",
     "shell.execute_reply": "2023-09-18T07:35:45.955383Z",
     "shell.execute_reply.started": "2023-09-18T07:35:45.948230Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pred['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T07:36:26.938248Z",
     "iopub.status.busy": "2023-09-18T07:36:26.937352Z",
     "iopub.status.idle": "2023-09-18T07:36:26.957246Z",
     "shell.execute_reply": "2023-09-18T07:36:26.956299Z",
     "shell.execute_reply.started": "2023-09-18T07:36:26.938204Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pred.to_csv('submission.csv',\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
