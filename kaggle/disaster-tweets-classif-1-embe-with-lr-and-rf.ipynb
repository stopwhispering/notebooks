{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Disaster Tweets - Classification #1 - Pretrained Embeddings with LR and RandomForest","metadata":{}},{"cell_type":"markdown","source":"This kernel includes codes and ideas from kernels below.\n- https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Running on {DEVICE}')\n\n# running in google colab\nif 'google.colab' in str(get_ipython()):\n    BASE_PATH = './drive/MyDrive/Colab/data/'\n    BASE_PATH_PRETRAINED = './drive/MyDrive/Colab/pretrained/'\n    from google.colab import drive\n    drive.mount('/content/drive')\n    import nltk\n    nltk.download('stopwords')\n    !pip install tokenizers\n\n# running interactively in kaggle\nelif get_ipython().config.IPKernelApp.connection_file.startswith('/root/.local/share'):\n    BASE_PATH = '/kaggle/input/'\n    BASE_PATH_PRETRAINED = '/kaggle/input/'\n    \n# running as background job in kaggle\nelif 'SHLVL' in os.environ:\n    BASE_PATH = '/kaggle/input/'\n    BASE_PATH_PRETRAINED = '/kaggle/input/'\n\nelse:\n    BASE_PATH = '../data/'\n    BASE_PATH_PRETRAINED = '../pretrained/'","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:31.843029Z","iopub.execute_input":"2023-09-18T07:51:31.843460Z","iopub.status.idle":"2023-09-18T07:51:34.890840Z","shell.execute_reply.started":"2023-09-18T07:51:31.843426Z","shell.execute_reply":"2023-09-18T07:51:34.889674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport pprint\nimport string\nfrom collections import Counter, defaultdict\nimport locale\nlocale.setlocale(locale.LC_ALL, locale='')  # for thousands separator via ... print(f'{value:n}')\"\nimport re\nfrom pprint import pprint\nimport requests\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchtext\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.ticker\nfrom matplotlib.axes._axes import Axes\nimport nltk\nimport numpy as np\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFD, StripAccents, Lowercase\n#from tokenizers import Tokenizer\n#from tokenizers.models import WordPiece\n#from tokenizers.trainers import WordPieceTrainer\n#from tokenizers.processors import TemplateProcessing\n# from transformers import BertModel\nimport gensim\n\n\nmy_seed = 42\nrandom.seed(my_seed)\ntorch.manual_seed(my_seed);","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:34.893177Z","iopub.execute_input":"2023-09-18T07:51:34.893854Z","iopub.status.idle":"2023-09-18T07:51:57.918881Z","shell.execute_reply.started":"2023-09-18T07:51:34.893811Z","shell.execute_reply":"2023-09-18T07:51:57.917683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df_source = pd.read_csv(BASE_PATH + 'nlp-getting-started/train.csv')\ndf_randomized = df_source.sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:57.920850Z","iopub.execute_input":"2023-09-18T07:51:57.921663Z","iopub.status.idle":"2023-09-18T07:51:57.984989Z","shell.execute_reply.started":"2023-09-18T07:51:57.921618Z","shell.execute_reply":"2023-09-18T07:51:57.983713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_VAL = int(len(df_randomized) * 0.3)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:57.987850Z","iopub.execute_input":"2023-09-18T07:51:57.988538Z","iopub.status.idle":"2023-09-18T07:51:57.993699Z","shell.execute_reply.started":"2023-09-18T07:51:57.988500Z","shell.execute_reply":"2023-09-18T07:51:57.992488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_source = df_randomized[:-NUM_VAL]\ndf_val_source = df_randomized[-NUM_VAL:]\ndf_train_source","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:57.995355Z","iopub.execute_input":"2023-09-18T07:51:57.995957Z","iopub.status.idle":"2023-09-18T07:51:58.032643Z","shell.execute_reply.started":"2023-09-18T07:51:57.995912Z","shell.execute_reply":"2023-09-18T07:51:58.031820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replace NaN","metadata":{}},{"cell_type":"code","source":"def replace_nan(df: pd.DataFrame) -> pd.DataFrame:\n    df_ = df.copy()\n    df_['keyword'] = df_['keyword'].fillna('')\n    df_['location'] = df_['location'].fillna('')\n    return df_\n\ndf_train = replace_nan(df_train_source)\ndf_val = replace_nan(df_val_source)\ndf_train","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.034009Z","iopub.execute_input":"2023-09-18T07:51:58.034355Z","iopub.status.idle":"2023-09-18T07:51:58.063488Z","shell.execute_reply.started":"2023-09-18T07:51:58.034325Z","shell.execute_reply":"2023-09-18T07:51:58.062323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We will ignore keyword and location (see EDA workbook) and only use the tweets themselves.","metadata":{}},{"cell_type":"code","source":"ser_train = df_train['text']","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.065225Z","iopub.execute_input":"2023-09-18T07:51:58.065959Z","iopub.status.idle":"2023-09-18T07:51:58.071354Z","shell.execute_reply.started":"2023-09-18T07:51:58.065918Z","shell.execute_reply":"2023-09-18T07:51:58.070307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize Strings\nusing HuggingFace's Normalizer","metadata":{}},{"cell_type":"code","source":"normalizer = normalizers.Sequence([\n    NFD(),   # NFD unicode normalization\n    Lowercase(),\n    StripAccents()  #\n])\n\nprint(\"Some string normalizing examples:\")\nunnormalized = [\"Héllò hôw are ü?\", \"éàù\", \"kožušček\", \"François\"]\nfor s in unnormalized:\n    print(f'{s :<30} -> {normalizer.normalize_str(s)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.072771Z","iopub.execute_input":"2023-09-18T07:51:58.073328Z","iopub.status.idle":"2023-09-18T07:51:58.091994Z","shell.execute_reply.started":"2023-09-18T07:51:58.073279Z","shell.execute_reply":"2023-09-18T07:51:58.090902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ser_train = ser_train.apply(normalizer.normalize_str)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.095007Z","iopub.execute_input":"2023-09-18T07:51:58.095472Z","iopub.status.idle":"2023-09-18T07:51:58.189218Z","shell.execute_reply.started":"2023-09-18T07:51:58.095428Z","shell.execute_reply":"2023-09-18T07:51:58.187924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Strip Hashtags","metadata":{}},{"cell_type":"code","source":"REGEX_HASHTAG_BEFORE = r'(?<!\\S)#(\\S+)'\nREGEX_HASHTAG_AFTER = r'\\1'\ndef strip_hashtags(tweet: str) -> str:\n    return re.sub(REGEX_HASHTAG_BEFORE, REGEX_HASHTAG_AFTER, tweet)\n\nexample_sentence = 'Our Deeds are the Reason of this #earthquake'\nprint('Example:')\nprint(f'{example_sentence :<80} -> {strip_hashtags(example_sentence)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.195323Z","iopub.execute_input":"2023-09-18T07:51:58.195758Z","iopub.status.idle":"2023-09-18T07:51:58.204077Z","shell.execute_reply.started":"2023-09-18T07:51:58.195725Z","shell.execute_reply":"2023-09-18T07:51:58.202997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ser_train = ser_train.apply(strip_hashtags)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.205606Z","iopub.execute_input":"2023-09-18T07:51:58.205956Z","iopub.status.idle":"2023-09-18T07:51:58.295720Z","shell.execute_reply.started":"2023-09-18T07:51:58.205927Z","shell.execute_reply":"2023-09-18T07:51:58.294327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Punctuation","metadata":{}},{"cell_type":"code","source":"punct = re.compile(r'[^\\w\\s]')\n\ndef remove_punctuations(text: str) -> str:\n    return punct.sub(r'', text)\n\nser_train = ser_train.apply(strip_hashtags)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.297496Z","iopub.execute_input":"2023-09-18T07:51:58.298698Z","iopub.status.idle":"2023-09-18T07:51:58.377084Z","shell.execute_reply.started":"2023-09-18T07:51:58.298652Z","shell.execute_reply":"2023-09-18T07:51:58.376207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Americanize\n⚠ This actually transforms a lot as we tokenize","metadata":{}},{"cell_type":"code","source":"dl_url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\nbritish_to_american_map = requests.get(dl_url).json()\nlen(british_to_american_map)\nprint(british_to_american_map['colour'])\nprint(british_to_american_map['traumatise'])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.378797Z","iopub.execute_input":"2023-09-18T07:51:58.379150Z","iopub.status.idle":"2023-09-18T07:51:58.561473Z","shell.execute_reply.started":"2023-09-18T07:51:58.379117Z","shell.execute_reply":"2023-09-18T07:51:58.560317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def americanize(text: str):\n    tokenized = [british_to_american_map[w] if w in british_to_american_map\n                        else w for w in nltk.tokenize.word_tokenize(text)]\n    return ' '.join(tokenized)\n\nser_train = ser_train.apply(americanize)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:51:58.563141Z","iopub.execute_input":"2023-09-18T07:51:58.563729Z","iopub.status.idle":"2023-09-18T07:52:00.384922Z","shell.execute_reply.started":"2023-09-18T07:51:58.563696Z","shell.execute_reply":"2023-09-18T07:52:00.383705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Stopwords\n⚠ This actually transforms a lot as we tokenize","metadata":{}},{"cell_type":"code","source":"stop_words = set(nltk.corpus.stopwords.words('english'))\nprint(f'Loaded {len(stop_words)} NLTK Stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:00.386299Z","iopub.execute_input":"2023-09-18T07:52:00.386632Z","iopub.status.idle":"2023-09-18T07:52:00.396566Z","shell.execute_reply.started":"2023-09-18T07:52:00.386603Z","shell.execute_reply":"2023-09-18T07:52:00.395343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(text: str):\n    tokenized = [word for word in nltk.tokenize.word_tokenize(text) if word.lower() not in stop_words]\n    return ' '.join(tokenized)\n\nser_train = ser_train.apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:00.398788Z","iopub.execute_input":"2023-09-18T07:52:00.399562Z","iopub.status.idle":"2023-09-18T07:52:02.301595Z","shell.execute_reply.started":"2023-09-18T07:52:00.399521Z","shell.execute_reply":"2023-09-18T07:52:02.300207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spelling","metadata":{}},{"cell_type":"code","source":"# rectify some common spelling mistakes\nspelling_dict = {\n    'didnt': \"didn't\",\n    'doesnt': \"doesn't\",\n    'isnt': \"isn't\",\n    'aint': \"ain't\",\n    'wasnt': \"wasn't\",\n    'shouldnt': \"shoudn't\",\n    'im': \"i'm\",\n}\n\ndef rectify_spelling(text: str):\n    tokenized = nltk.tokenize.word_tokenize(text)\n    corrected = [spelling_dict.get(w, w) for w in tokenized]\n    return ' '.join(corrected)\n\nser_train = ser_train.apply(rectify_spelling)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:02.303567Z","iopub.execute_input":"2023-09-18T07:52:02.304037Z","iopub.status.idle":"2023-09-18T07:52:04.034130Z","shell.execute_reply.started":"2023-09-18T07:52:02.303988Z","shell.execute_reply":"2023-09-18T07:52:04.032828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replace Contractions\nReplace some abbreviated pronouns with full forms","metadata":{}},{"cell_type":"code","source":"contraction_mapping = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"couldn't\": \"could not\",\n    \"could've\": \"could have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'll\": \"he will\",\n    \"here's\": \"here is\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"i'd\": \"i would\",\n    \"I'd've\": \"I would have\",\n    \"i'd've\": \"i would have\",\n    \"I'll\": \"I will\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"I'll've\": \"I will have\",\n    \"i'm\": \"i am\",\n    \"I'm\": \"I am\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"I've\": \"I have\",\n    \"i've\": \"i have\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"might've\": \"might have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"must've\": \"must have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"n't\": \"not\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"should've\": \"should have\",\n    \"so's\": \"so as\",\n    \"so've\": \"so have\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"this's\": \"this is\",\n    \"to've\": \"to have\",\n    \"'ve\": \"have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"weren't\": \"were not\",\n    \"we've\": \"we have\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"would've\": \"would have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:04.036283Z","iopub.execute_input":"2023-09-18T07:52:04.037031Z","iopub.status.idle":"2023-09-18T07:52:04.055148Z","shell.execute_reply.started":"2023-09-18T07:52:04.036999Z","shell.execute_reply":"2023-09-18T07:52:04.053651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_contraction(tweet: str):\n    for contraction, full_form in contraction_mapping.items():\n        tweet = re.sub(contraction, full_form, tweet)\n    return tweet\n\nser_train = ser_train.apply(replace_contraction)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:04.056413Z","iopub.execute_input":"2023-09-18T07:52:04.056727Z","iopub.status.idle":"2023-09-18T07:52:05.074803Z","shell.execute_reply.started":"2023-09-18T07:52:04.056701Z","shell.execute_reply":"2023-09-18T07:52:05.073466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Transformation Pipeline to Train and Validation","metadata":{}},{"cell_type":"code","source":"ser_train = (df_train['text']\n             .apply(normalizer.normalize_str)  # normalize\n             .apply(strip_hashtags)\n             .apply(remove_punctuations)\n             .apply(remove_stopwords)\n             .apply(americanize)\n             .apply(rectify_spelling)\n             .apply(replace_contraction)\n            )\n\nser_val = (df_val['text']\n             .apply(normalizer.normalize_str)\n             .apply(strip_hashtags)\n             .apply(remove_punctuations)\n             .apply(remove_stopwords)\n             .apply(americanize)\n             .apply(rectify_spelling)\n             .apply(replace_contraction)\n            )","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:05.076334Z","iopub.execute_input":"2023-09-18T07:52:05.076667Z","iopub.status.idle":"2023-09-18T07:52:12.035319Z","shell.execute_reply.started":"2023-09-18T07:52:05.076639Z","shell.execute_reply":"2023-09-18T07:52:12.034058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2vec Pretrained Model","metadata":{}},{"cell_type":"code","source":"path = BASE_PATH_PRETRAINED + 'googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\nwv = gensim.models.KeyedVectors.load_word2vec_format(path, \n                                                     binary=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:52:12.036981Z","iopub.execute_input":"2023-09-18T07:52:12.037507Z","iopub.status.idle":"2023-09-18T07:53:43.386330Z","shell.execute_reply.started":"2023-09-18T07:52:12.037468Z","shell.execute_reply":"2023-09-18T07:53:43.384998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the canonical example...\nwv.most_similar(positive=['woman', 'king'], negative=['man'])[:3]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:43.387914Z","iopub.execute_input":"2023-09-18T07:53:43.388291Z","iopub.status.idle":"2023-09-18T07:53:45.888179Z","shell.execute_reply.started":"2023-09-18T07:53:43.388250Z","shell.execute_reply":"2023-09-18T07:53:45.886728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding Coverage","metadata":{}},{"cell_type":"markdown","source":"Embedding Coverage tells how much percentage of the words in our data are covered by the vocabulary. Words that are not covered by the Embeddings vocab are basically not used for classification. So we should make sure to include as much as possible using preprocessing techniques.","metadata":{}},{"cell_type":"code","source":"def compute_embeddings_coverage(ser: pd.Series):\n\n    # get <<all>> and <<all distinct>> words in train data\n    flat_words = [word for sentence in ser for word in nltk.word_tokenize(sentence)]\n    distinct_words = set(flat_words)\n    print(f'Found a total of {len(flat_words) :n} words, with {len(distinct_words) :n} distinct words.')\n\n    missing_words = defaultdict(int)\n\n    for word in flat_words:\n        if word not in wv.key_to_index:\n            missing_words[word] += 1\n\n    print(f'Found {len(missing_words)} words missing in embeddings.')\n\n    embeddings_vocab_coverage = (len(distinct_words) - len(missing_words)) / len(distinct_words)\n    total_words_missing = sum(count for count in missing_words.values())\n    embeddings_text_coverage = (len(flat_words) - total_words_missing) / len(flat_words)\n\n    return missing_words, embeddings_vocab_coverage, embeddings_text_coverage","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:45.889820Z","iopub.execute_input":"2023-09-18T07:53:45.890967Z","iopub.status.idle":"2023-09-18T07:53:45.902049Z","shell.execute_reply.started":"2023-09-18T07:53:45.890923Z","shell.execute_reply":"2023-09-18T07:53:45.900831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_words, embeddings_vocab_coverage, embeddings_text_coverage = compute_embeddings_coverage(ser=ser_train)\nprint(f'Embeddings Vocab Coverage: {embeddings_vocab_coverage :.2%}')\nprint(f'Embeddings Text Coverage: {embeddings_text_coverage :.2%}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:45.903974Z","iopub.execute_input":"2023-09-18T07:53:45.904320Z","iopub.status.idle":"2023-09-18T07:53:47.110445Z","shell.execute_reply.started":"2023-09-18T07:53:45.904294Z","shell.execute_reply":"2023-09-18T07:53:47.109318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_words_list = [(word, count) for word, count in missing_words.items()]\nsorted_missing_words = sorted(missing_words_list, key=lambda x: -x[1])\nprint(f'Most frequently used missing words:')\npprint(sorted_missing_words[:25])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:47.111827Z","iopub.execute_input":"2023-09-18T07:53:47.112164Z","iopub.status.idle":"2023-09-18T07:53:47.170065Z","shell.execute_reply.started":"2023-09-18T07:53:47.112136Z","shell.execute_reply":"2023-09-18T07:53:47.168734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Vocabulary and Custom Embeddings","metadata":{"id":"ceae6139-1392-4820-b303-5719e973e29d"}},{"cell_type":"code","source":"# we won't encode all words, but only the n most common words\nflat_words = [word for text in ser_train for word in nltk.tokenize.word_tokenize(text)]\ndistinct_words = set(flat_words)\nprint(f'{len(distinct_words)} distinct words.')","metadata":{"id":"5a579f15-9e3b-4e65-86fb-050de116b0d9","outputId":"35bcf018-7a51-478c-e327-179323107020","execution":{"iopub.status.busy":"2023-09-18T07:53:47.171464Z","iopub.execute_input":"2023-09-18T07:53:47.171766Z","iopub.status.idle":"2023-09-18T07:53:48.366802Z","shell.execute_reply.started":"2023-09-18T07:53:47.171739Z","shell.execute_reply":"2023-09-18T07:53:48.365531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word to index\nwords_with_embeddings = [w for w in distinct_words if w in wv.key_to_index]\nprint(f'{len(words_with_embeddings)} words with pretrained word vectors.')","metadata":{"id":"f0363f74-556c-483f-acb9-fbe19860010a","outputId":"e4153093-07db-4b09-e3e4-4ecb88c16188","execution":{"iopub.status.busy":"2023-09-18T07:53:48.368094Z","iopub.execute_input":"2023-09-18T07:53:48.371381Z","iopub.status.idle":"2023-09-18T07:53:48.390357Z","shell.execute_reply.started":"2023-09-18T07:53:48.371347Z","shell.execute_reply":"2023-09-18T07:53:48.389113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words_without_embeddings = [w for w in distinct_words if w not in wv.key_to_index]\nprint(f'{len(words_without_embeddings)} words without pretrained word vectors. We will ignore them.')","metadata":{"id":"fe3f3cea-f83d-4913-b747-4978e573b837","outputId":"f541582b-8ebe-4377-ff42-4f5869a4b7cc","execution":{"iopub.status.busy":"2023-09-18T07:53:48.400711Z","iopub.execute_input":"2023-09-18T07:53:48.401062Z","iopub.status.idle":"2023-09-18T07:53:48.417723Z","shell.execute_reply.started":"2023-09-18T07:53:48.401034Z","shell.execute_reply":"2023-09-18T07:53:48.416263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_to_index = {token: index for index, token in enumerate(words_with_embeddings)}\nindex_to_token = {index: token for token, index in token_to_index.items()}\n\n# wrapper for token-to-index mapping\nvocab = torchtext.vocab.vocab(token_to_index)","metadata":{"id":"f1d9d744-135a-4e53-a1fc-b5080cf99244","execution":{"iopub.status.busy":"2023-09-18T07:53:48.420066Z","iopub.execute_input":"2023-09-18T07:53:48.420534Z","iopub.status.idle":"2023-09-18T07:53:48.522789Z","shell.execute_reply.started":"2023-09-18T07:53:48.420478Z","shell.execute_reply":"2023-09-18T07:53:48.521572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create initiual embeddings with all-zeros for our 80.000 words with 300 dimensions (like pretrained embeddings)\nembeddings = torch.zeros(len(token_to_index), wv.vectors.shape[1])\nembeddings.shape","metadata":{"id":"ce7b0876-d1fb-498f-bea3-5ae5aa9d1ee3","outputId":"593afc16-8ad6-408d-98fe-5d637ca4acd0","execution":{"iopub.status.busy":"2023-09-18T07:53:48.524397Z","iopub.execute_input":"2023-09-18T07:53:48.525606Z","iopub.status.idle":"2023-09-18T07:53:48.567283Z","shell.execute_reply.started":"2023-09-18T07:53:48.525552Z","shell.execute_reply":"2023-09-18T07:53:48.566093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we use the known words' embeddings in our model\nindices_with_embeddings = [token_to_index[w] for w in words_with_embeddings]\n\n# map from \"new\" to \"old\" index (i.e. pretrained index)\nindex_to_pretrained_index = {index: wv.key_to_index[index_to_token[index]] for index in indices_with_embeddings}","metadata":{"id":"a28e8a8d-c52a-42c5-85f3-37e6586c38fe","execution":{"iopub.status.busy":"2023-09-18T07:53:48.568947Z","iopub.execute_input":"2023-09-18T07:53:48.570144Z","iopub.status.idle":"2023-09-18T07:53:48.590754Z","shell.execute_reply.started":"2023-09-18T07:53:48.570101Z","shell.execute_reply":"2023-09-18T07:53:48.589329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, pretrained_index in index_to_pretrained_index.items():\n    embeddings[index] = torch.Tensor(wv.vectors[pretrained_index])  # ndarray to tensor","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:48.592102Z","iopub.execute_input":"2023-09-18T07:53:48.592893Z","iopub.status.idle":"2023-09-18T07:53:48.750324Z","shell.execute_reply.started":"2023-09-18T07:53:48.592860Z","shell.execute_reply":"2023-09-18T07:53:48.749101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:48.751729Z","iopub.execute_input":"2023-09-18T07:53:48.752133Z","iopub.status.idle":"2023-09-18T07:53:48.823942Z","shell.execute_reply.started":"2023-09-18T07:53:48.752094Z","shell.execute_reply":"2023-09-18T07:53:48.823121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"def tokenize_if_in_vocab(text: str) -> str:\n    return [w for w in nltk.tokenize.word_tokenize(text) if w in vocab]\n\ntokenized_train = ser_train.apply(tokenize_if_in_vocab)\nprint(f'tokenized_train is a {type(tokenized_train)} of shape {tokenized_train.shape}')\n\ntokenized_val = ser_val.apply(tokenize_if_in_vocab)\nprint(f'tokenized_val is a {type(tokenized_val)} of shape {tokenized_val.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:48.824982Z","iopub.execute_input":"2023-09-18T07:53:48.825987Z","iopub.status.idle":"2023-09-18T07:53:50.631823Z","shell.execute_reply.started":"2023-09-18T07:53:48.825946Z","shell.execute_reply":"2023-09-18T07:53:50.630498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute Average Feature Vector per Tweet","metadata":{}},{"cell_type":"code","source":"def compute_average_feature_vector(tokens: list[str]) -> torch.tensor:\n    feature_vec_sum = np.zeros((300, ), \n                               dtype='float32')\n    for token in tokens:\n        index = token_to_index[token]\n        feature_vec_current_token = embeddings[index]\n        feature_vec_sum = np.add(feature_vec_sum, feature_vec_current_token)  # returns a tensor!\n\n    feature_vec_avg = np.divide(feature_vec_sum, len(tokens)) if len(tokens) > 0 else torch.tensor(feature_vec_sum)\n    return feature_vec_avg  # [300]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:50.633260Z","iopub.execute_input":"2023-09-18T07:53:50.634070Z","iopub.status.idle":"2023-09-18T07:53:50.640262Z","shell.execute_reply.started":"2023-09-18T07:53:50.634038Z","shell.execute_reply":"2023-09-18T07:53:50.639120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_average_feature_vectors = tokenized_train.apply(compute_average_feature_vector)\nval_average_feature_vectors = tokenized_val.apply(compute_average_feature_vector)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:50.641772Z","iopub.execute_input":"2023-09-18T07:53:50.642068Z","iopub.status.idle":"2023-09-18T07:53:52.715074Z","shell.execute_reply.started":"2023-09-18T07:53:50.642042Z","shell.execute_reply":"2023-09-18T07:53:52.714164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_average_feature_vectors is a pd.Series of size 7613 with each element being a Tensor of size [300]\n# we need to convert that to a np.Array of size (7613, 300)\nser_train_arr = train_average_feature_vectors.apply(lambda x: x.numpy())  # series of arrays\nx_train = np.stack(ser_train_arr.values)  # array of size (7613, 300)\n\nser_val_arr = val_average_feature_vectors.apply(lambda x: x.numpy())\nx_val = np.stack(ser_val_arr.values)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:52.717024Z","iopub.execute_input":"2023-09-18T07:53:52.717484Z","iopub.status.idle":"2023-09-18T07:53:53.071689Z","shell.execute_reply.started":"2023-09-18T07:53:52.717443Z","shell.execute_reply":"2023-09-18T07:53:53.070415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = df_train_source['target']\ny_val = df_val_source['target']\n\nassert x_train.shape[0] == y_train.shape[0]\nassert x_val.shape[0] == y_val.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:53.073723Z","iopub.execute_input":"2023-09-18T07:53:53.074364Z","iopub.status.idle":"2023-09-18T07:53:53.082727Z","shell.execute_reply.started":"2023-09-18T07:53:53.074316Z","shell.execute_reply":"2023-09-18T07:53:53.081323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classify using LogisticRegression and RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr = LogisticRegression()  # we'll try with the default hyperparams\nlr.fit(x_train, y_train)\n\ny_pred_val = lr.predict(x_val)\n\nprint(f'Validation F1-Score: {metrics.f1_score(y_true=y_val, y_pred=y_pred_val)}')\nprint(f'Validation Accuracy: {metrics.accuracy_score(y_true=y_val, y_pred=y_pred_val)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:53.084433Z","iopub.execute_input":"2023-09-18T07:53:53.084882Z","iopub.status.idle":"2023-09-18T07:53:53.215623Z","shell.execute_reply.started":"2023-09-18T07:53:53.084840Z","shell.execute_reply":"2023-09-18T07:53:53.214485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\n#y_pred_train = rfc.predict(x_train)\ny_pred_val = rfc.predict(x_val)\n\nprint(f'Validation F1-Score: {metrics.f1_score(y_true=y_val, y_pred=y_pred_val)}')\nprint(f'Validation Accuracy: {metrics.accuracy_score(y_true=y_val, y_pred=y_pred_val)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:53:53.217387Z","iopub.execute_input":"2023-09-18T07:53:53.218305Z","iopub.status.idle":"2023-09-18T07:54:01.942997Z","shell.execute_reply.started":"2023-09-18T07:53:53.218260Z","shell.execute_reply":"2023-09-18T07:54:01.941784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"df_test_source = pd.read_csv(BASE_PATH + 'nlp-getting-started/test.csv')\ndf_test = replace_nan(df_test_source)\ndf_test  # no target col","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:54:01.944592Z","iopub.execute_input":"2023-09-18T07:54:01.945011Z","iopub.status.idle":"2023-09-18T07:54:01.984160Z","shell.execute_reply.started":"2023-09-18T07:54:01.944971Z","shell.execute_reply":"2023-09-18T07:54:01.982997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ser_test = (df_test['text']\n             .apply(normalizer.normalize_str)\n             .apply(strip_hashtags)\n             .apply(remove_punctuations)\n             .apply(remove_stopwords)\n             .apply(americanize)\n             .apply(rectify_spelling)\n             .apply(replace_contraction)\n            )\n\nser_test","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:54:01.985940Z","iopub.execute_input":"2023-09-18T07:54:01.986695Z","iopub.status.idle":"2023-09-18T07:54:04.929085Z","shell.execute_reply.started":"2023-09-18T07:54:01.986653Z","shell.execute_reply":"2023-09-18T07:54:04.927921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test = ser_test.apply(tokenize_if_in_vocab)\ntest_average_feature_vectors = tokenized_test.apply(compute_average_feature_vector)\nser_test_arr = test_average_feature_vectors.apply(lambda x: x.numpy())  # series of arrays\nx_test = np.stack(ser_test_arr.values)  # array of size (3263, 300)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:54:41.226597Z","iopub.execute_input":"2023-09-18T07:54:41.226988Z","iopub.status.idle":"2023-09-18T07:54:42.841051Z","shell.execute_reply.started":"2023-09-18T07:54:41.226959Z","shell.execute_reply":"2023-09-18T07:54:42.839807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the LogisticRegressor scored slightly better than the RandomForestClassifier in terms of F1-score\ny_pred = lr.predict(x_test)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:56:02.588128Z","iopub.execute_input":"2023-09-18T07:56:02.588572Z","iopub.status.idle":"2023-09-18T07:56:02.603144Z","shell.execute_reply.started":"2023-09-18T07:56:02.588537Z","shell.execute_reply":"2023-09-18T07:56:02.601847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ser_pred = pd.Series(y_pred)\ndf_pred = pd.DataFrame({'id': df_test['id'],\n                       'target': ser_pred})\ndf_pred","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:56:49.924381Z","iopub.execute_input":"2023-09-18T07:56:49.924776Z","iopub.status.idle":"2023-09-18T07:56:49.939184Z","shell.execute_reply.started":"2023-09-18T07:56:49.924747Z","shell.execute_reply":"2023-09-18T07:56:49.938025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:56:59.091138Z","iopub.execute_input":"2023-09-18T07:56:59.091983Z","iopub.status.idle":"2023-09-18T07:56:59.104639Z","shell.execute_reply.started":"2023-09-18T07:56:59.091942Z","shell.execute_reply":"2023-09-18T07:56:59.103528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred.to_csv('submission.csv',\n               index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:57:07.330071Z","iopub.execute_input":"2023-09-18T07:57:07.330508Z","iopub.status.idle":"2023-09-18T07:57:07.346454Z","shell.execute_reply.started":"2023-09-18T07:57:07.330477Z","shell.execute_reply":"2023-09-18T07:57:07.345257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}