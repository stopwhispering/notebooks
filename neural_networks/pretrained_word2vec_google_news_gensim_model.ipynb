{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2629c2d5-77a8-4fed-8bc2-792ace0ea535",
   "metadata": {},
   "source": [
    "# Gensim - Working with Word2vec pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f89d84-912a-482c-8f50-56761587bcc8",
   "metadata": {},
   "source": [
    "Subject: Download and work with pretrained Work2vec embeddings.\n",
    "\n",
    "Data: google-news model, trained with Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. \n",
    "\n",
    "Procedure:\n",
    "- Print list of pretrained models from Gensim\n",
    "- Download model trained with Google-News dataset (to be executed in console)\n",
    "- Save that model locally\n",
    "- Load that model from local file, preferably with limited vocab\n",
    "- Load the model's vocab in Pytorch as torchtext.vocab.Vocab\n",
    "- Load the model's word vectors into a torch.nn.Embedding layer for usage in torch model\n",
    "\n",
    "Others:\n",
    "- Support for Colab for saving/loading from Google Drive\n",
    "\n",
    "Sources:\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- https://saturncloud.io/blog/pytorch-gensim-how-do-i-load-pretrained-word-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745716b-56a8-49c8-a289-e3473c68880f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a615f7a-b76f-490b-8625-71d32351733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB := 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = './drive/MyDrive/Colab/'\n",
    "\n",
    "else:\n",
    "    BASE_PATH = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f995494-b013-4815-bf9f-39d5e0cd4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, locale='')  # for thousands separator via ... print(f'{value:n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e167d8-eda8-47d0-bcf5-975e163cc66c",
   "metadata": {},
   "source": [
    "## gensim's Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95413ad1-3349-44aa-8830-72b9adb6ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c632d8c0-ff0f-4da7-bf96-df07cb676f64",
   "metadata": {},
   "source": [
    "### List gensim pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f3d61-8f49-4876-b9fd-3f4aff127ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48128a9b-0d36-48f0-90e0-e76b7f709beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(gensim.downloader.info()['models']['word2vec-google-news-300'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea8759-cdde-480f-93ce-12d09580d0f7",
   "metadata": {},
   "source": [
    "### Download pretrained word vectors and save locally\n",
    "better execute in <b>console</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c1d58-6d1a-4bd7-bc00-25370fcb6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download might crash jupyter lab\n",
    "# might help\n",
    "# -> \"jupyter-lab --NotebookApp.iopub_data_rate_limit=1e10\"\n",
    "# -> \"jupyter-lab --NotebookApp.iopub_msg_rate_limit=1e10\"\n",
    "# but better download and save in console, then load from local file in notebook\n",
    "\n",
    "wv = downloader.load('word2vec-google-news-300')  # ~1.7 GB\n",
    "wv.save_word2vec_format(BASE_PATH + 'pretrained/word2vec-google-news-300.bin', binary=True)  # >3.5gb; takes few sec.  # see https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933ddbb-cd48-4739-a4dc-128e55d31fcf",
   "metadata": {},
   "source": [
    "### Load saved pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b4768ce-8152-45a6-956f-c10f7659316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved \n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(BASE_PATH + 'pretrained/word2vec-google-news-300.bin', binary=True)  # takes a few sec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fbdf3-95b3-4e42-a039-bdcf6750347b",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedc7fa7-3b39-44aa-a86f-3a36edfaabaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a8310-69d4-42da-8dfa-1ca80933eccc",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2887f9-009c-4788-a07c-96c8133aceef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa7ff21-685b-4e49-aa6a-da875025f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3.000.000 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size: {len(wv.key_to_index):n} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e37cb2fd-5ad1-45d7-bbdc-ea757f08f8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 100.000 words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454),\n",
       " ('royal_palace', 0.5087166428565979)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With over 3GB in RAM, the Google-News-embeddings are too large to be convenient to work with. \n",
    "# Load only the first 100.000 (of 3.000.000) words:\n",
    "wv_limited = gensim.models.KeyedVectors.load_word2vec_format(BASE_PATH + 'pretrained/word2vec-google-news-300.bin',\n",
    "                                                     binary=True,\n",
    "                                                     limit=100000)\n",
    "print(f'Vocab size: {len(wv_limited.key_to_index):n} words.')\n",
    "\n",
    "wv_limited.most_similar(positive=['woman', 'king'], negative=['man'])  # Notice that 'Queen_Consort' is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c3ef51-caa6-4697-8e64-b268bc32d190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20.000 words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('monarchy', 0.5087411999702454),\n",
       " ('throne', 0.5005807876586914),\n",
       " ('royal', 0.493820458650589),\n",
       " ('ruler', 0.4909275770187378),\n",
       " ('kingdom', 0.4550568163394928),\n",
       " ('palace', 0.45176562666893005),\n",
       " ('Princess', 0.4375406801700592)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_more_limited = gensim.models.KeyedVectors.load_word2vec_format(BASE_PATH + 'pretrained/word2vec-google-news-300.bin',\n",
    "                                                     binary=True,\n",
    "                                                     limit=20000)\n",
    "print(f'Vocab size: {len(wv_more_limited.key_to_index):n} words.')\n",
    "\n",
    "wv_more_limited.most_similar(positive=['woman', 'king'], negative=['man'])  # With more exotic words missing, the results seem more plausible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47789df-09f8-4a7e-a05a-1b90a3da8f67",
   "metadata": {},
   "source": [
    "## Use gensim embeddings in PyTorchðŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cce5b0-04a5-47fa-be6b-50c4b027314c",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a9f9186-1f98-4e03-98ad-69544a3b04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2c55bd6-1f3e-435c-ae9e-e24bdd11cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vocab.vocab(wv_more_limited.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "524aeffa-14a1-46f0-bf8b-1ea4f1070561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6146"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998f4d3-e693-4b3d-9314-1d9d388d8481",
   "metadata": {},
   "source": [
    "### Embeddings from Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65f07283-6ad9-4ab0-b24a-9d6c5ba4917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1c796b-b7bf-4ed6-8c3c-ac3371098a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".vectors is a <class 'numpy.ndarray'> of size (20000, 300) with dtype float32.\n"
     ]
    }
   ],
   "source": [
    "# the vectors are simply an ndarray of size...\n",
    "print(f'.vectors is a {type(wv_more_limited.vectors)} of size {wv_more_limited.vectors.shape} with dtype {wv_more_limited.vectors.dtype}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4f90c9d-fd65-4d2b-a726-b8bf2f6c944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to <class 'torch.Tensor'> of size torch.Size([20000, 300]) with dtype torch.float32.\n"
     ]
    }
   ],
   "source": [
    "# convert to tensor\n",
    "pretrained_vectors = torch.tensor(wv_more_limited.vectors)\n",
    "print(f'Converted to {type(pretrained_vectors)} of size {pretrained_vectors.shape} with dtype {pretrained_vectors.dtype}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d625c16-b453-4bbf-957a-8e19bc1e3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(pretrained_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21be283-a5ae-4755-b430-ee8b4418afe8",
   "metadata": {},
   "source": [
    "You can then use this layer in your PyTorch model, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d7786fb-53ec-40e0-a006-7c85823bce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_vectors)\n",
    "        self.fc = nn.Linear(300, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output = self.fc(embedded.mean(dim=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcdf32-e777-496b-a214-1acb82a89232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
