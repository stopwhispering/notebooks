{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58672e7c-e8a9-4765-a45f-ca2192f0e437",
   "metadata": {},
   "source": [
    "# PyTorchüî•Embeddings - Build Word2vec CBOW Model (150 Novels Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e1269-e5c4-46ff-930d-343b8f2f7b82",
   "metadata": {},
   "source": [
    "Subject: Building a Word2vec-like CBOW Model to create embeddings for the dataset's words representing their near closeness to each other (in 100 embedding dimensions).\n",
    "\n",
    "Data: txtlab_Novel150_English (150 English novels from the 19th century)\n",
    "\n",
    "Procedure:\n",
    "- Tokenizing with nltk.tokenize.word_tokenize and nltk.corpus.stopwords\n",
    "- Creating contexts and targets from five words each: (01 34) with (2) as target\n",
    "- Tensorizing contexts and targets\n",
    "- Creating a vocabulary with torchtext.vocab.build_vocab_from_iterator\n",
    "- Creating a custom torch.utils.data.Dataset for a torch.utils.data.DataLoader\n",
    "- Word2vec-like CBOW model with torch.nn.module, torch.nn.Embedding, torch.nn.Linear, torch.nn.ReLU,  and torch.nn.LogSoftmax\n",
    "- Training with torch.nn.NLLLoss, torch.optim.SGD, and torch.optim.lr_scheduler.StepLR\n",
    "- Evaluation by finding some nearest words and playing with word vectors\n",
    "- Disappointing results (probably much more data required)\n",
    "\n",
    "Others:\n",
    "- CUDA support\n",
    "- working on Colab with Google Drive for saving/loading interim stages\n",
    "\n",
    "Sources used:\n",
    "- https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2066b-2f32-4c40-9776-a046eb7954f3",
   "metadata": {},
   "source": [
    "Tested on Colab with CODA for multiple parameters. LR, batch size, and scheduler seem to make no big difference. After 5 epochs, the loss is usually around 8.3. The tests for KING, QUEEN, etc. fail. The dataset seems not to be enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "446cb53e-d41f-4888-ab7b-0a2bc2befb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {DEVICE}')\n",
    "\n",
    "if IN_COLAB := 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  BASE_PATH = './drive/MyDrive/Colab/'\n",
    "  import nltk\n",
    "  nltk.download('stopwords')\n",
    "\n",
    "else:\n",
    "  BASE_PATH = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703e004-ba42-49b1-8628-a991ba43548e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7091360-4f40-4bad-ae60-fcc8404931d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German_Germany.1252'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, locale='')  # for thousands separator via ... print(f'{value:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6dc9ff0-d1dc-4bd0-bab7-34b8cec36548",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_METADATA_TABLE = './data/novel/txtlab_Novel150_English.csv'\n",
    "PATH_DIR_NOVELS = './data/novel/txtlab_Novel150_English/'\n",
    "\n",
    "metadata_tb = pd.read_csv(PATH_METADATA_TABLE)  # contains novel filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d725b75-e229-4a0c-83dc-47cb2796ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read list of 150 Novels.\n"
     ]
    }
   ],
   "source": [
    "novels: list[str] = []\n",
    "\n",
    "for filename in metadata_tb['filename']:\n",
    "    with open(PATH_DIR_NOVELS+filename, 'r', encoding=\"utf8\") as file_in:\n",
    "        novel = file_in.read()\n",
    "    \n",
    "    # add the whole novel text as single string\n",
    "    novels.append(novel)\n",
    "\n",
    "\n",
    "#todo\n",
    "#novels = novels[:2]\n",
    "\n",
    "print(f\"Read list of {len(novels) :n} Novels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b4c009-bc87-4177-8b31-582c088a9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Extract from 16th novel:\\n\\n', novels[15][1003:1105])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ffc51-08a6-4f1f-90b6-85cb5978a256",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For CBOW, we need training data with...\n",
    "- the two previous words and \n",
    "- the two next words as X\n",
    "- and the word in-between as target (y).\n",
    "\n",
    "Preprocessing:\n",
    "- We start with the whole novels as a string each.\n",
    "- We tokenize the words in each novel.\n",
    "- We swipe along the words, from index=2 to index=-3.\n",
    "- For each of those target words, we collect the two previous and the two next words as X.\n",
    "- We do that for each novel.\n",
    "- The results are collected in a list of tuples, each tuple containing the two previous and two next words in a tuple (of 4), and the target word as string.\n",
    "\n",
    "Note: Note tokenizing the sentences is not ideal! We don't do it here for the sake of simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f469f-c832-4f2b-9401-766e22f34fe6",
   "metadata": {},
   "source": [
    "### Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08d836a-e9aa-4f7c-8d30-93d82f4244f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db9db60-4729-4073-9b74-617725fa2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_OCCURRENCES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de60ba04-9629-44b9-9346-3436633d609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with a list of 150 Novels with an average size of 688.646 characters each.\n"
     ]
    }
   ],
   "source": [
    "average_size = round(np.average([len(novel) for novel in novels]))\n",
    "print(f\"Starting with a list of {len(novels)} Novels with an average size of {average_size :n} characters each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8174dc09-a48e-41f5-aa10-63e8833b3cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed an average of 25.383 punctuation characters. Remaining average characters per novel: 663.263.\n"
     ]
    }
   ],
   "source": [
    "# to remove punctuation\n",
    "# translator = str.maketrans('', '', string.punctuation + '‚Äú‚Äù-;'‚Äô)\n",
    "translator = str.maketrans('', '', string.punctuation + '‚Äú‚Äù‚Äô‚Ä¶')  # ‚Äî\n",
    "novels_without_punctuation =  [novel.translate(translator) for novel in novels]\n",
    "\n",
    "\n",
    "average_size_without_punctuation = round(np.average([len(novel) for novel in novels_without_punctuation]))\n",
    "print(f\"Removed an average of {average_size - average_size_without_punctuation :n} punctuation characters.\",\n",
    "      f\"Remaining average characters per novel: {average_size_without_punctuation :n}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef35e90b-1316-4218-8791-dda8c45390b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words in 150 novels with an average of 123.369 words per novel.\n"
     ]
    }
   ],
   "source": [
    "tokenized_novels_ = [nltk.tokenize.word_tokenize(novel.lower(), \n",
    "                                                language='english')\n",
    "                    for novel in novels_without_punctuation]\n",
    "\n",
    "average_num_words = round(np.average([len(novel) for novel in tokenized_novels_]))\n",
    "print(f'Tokenized words in {len(novels_without_punctuation) :n} novels with an average of {average_num_words :n} words per novel.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dddec98-2186-4688-a58e-0f8866b4bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining average of 59.212 words per novel after removing most common words (stopwords).\n"
     ]
    }
   ],
   "source": [
    "# Remove most common words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_novels = [[word for word in novel if word not in stop_words]\n",
    "                    for novel in tokenized_novels_]\n",
    "\n",
    "average_num_words = round(np.average([len(novel) for novel in tokenized_novels]))\n",
    "print(f'Remaining average of {average_num_words :n} words per novel after removing most common words (stopwords).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a9d1170-e734-4151-bf38-b8b322eb2b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining average of 58.341 words per novel after removing words with 2+ special characters.\n"
     ]
    }
   ],
   "source": [
    "# Remove words with special characters, e.g. '√©l√©gante', 'wicked‚Äìbase‚Äìever'\n",
    "# this would remove only those with 2+ special characters:\n",
    "#tokenized_novels = [[word for word in novel if not (special := re.findall(pattern='[^A-Za-z0-9.]+', string=word)) or len(special[0]) < 2  ]\n",
    "#                    for novel in tokenized_novels]\n",
    "tokenized_novels = [[word for word in novel if not (special := re.findall(pattern='[^A-Za-z0-9.]+', string=word))]\n",
    "                    for novel in tokenized_novels]\n",
    "average_num_words = round(np.average([len(novel) for novel in tokenized_novels]))\n",
    "print(f'Remaining average of {average_num_words :n} words per novel after removing words with 2+ special characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a0954e-9b44-4e14-8c6f-54f39d7f6881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining average of 56.038 words per novel after removing words below minimum occurrence count.\n"
     ]
    }
   ],
   "source": [
    "# Remove words that have a word occurrence below the threshold\n",
    "flat_words = [item for sublist in tokenized_novels for item in sublist]\n",
    "words_counter = Counter(flat_words)\n",
    "\n",
    "tokenized_novels = [[word for word in novel if words_counter[word] >= MIN_WORD_OCCURRENCES]\n",
    "                    for novel in tokenized_novels]\n",
    "\n",
    "average_num_words = round(np.average([len(novel) for novel in tokenized_novels]))\n",
    "print(f'Remaining average of {average_num_words :n} words per novel after removing words below minimum occurrence count.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eb676-2053-4450-86b4-ac55c709f26d",
   "metadata": {},
   "source": [
    "### Create Context Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c12c21e0-6379-4873-8472-992a8e09344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected context training data of size 8.405.037 with 4 context words and a target word each. \n"
     ]
    }
   ],
   "source": [
    "contexts: list[tuple[tuple[str, str, str, str], str]] = []\n",
    "\n",
    "for novel in tokenized_novels:\n",
    "\n",
    "    novel_contexts = []\n",
    "    for i in range(2, len(novel) - 2):\n",
    "        context = (novel[i - 2], \n",
    "                   novel[i - 1],\n",
    "                   novel[i + 1], \n",
    "                   novel[i + 2])\n",
    "        target = novel[i]\n",
    "        contexts.append((context, target))\n",
    "\n",
    "print(f\"Collected context training data of size {len(contexts) :n} with 4 context words and a target word each. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4bf837-1a41-4c0d-a3e7-71a40893ef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of first novel: \n",
      "\n",
      "\n",
      "AUTHOR‚ÄôS INTRODUCTION\n",
      "\n",
      "\n",
      "\n",
      "My dog had made a point on a piece\n",
      "\n",
      "First tokens: ['authors', 'introduction', 'dog', 'made', 'point', 'piece', 'led', 'curate']\n",
      "\n",
      "Context 0: (('authors', 'introduction', 'made', 'point'), 'dog')\n",
      "Context 1: (('introduction', 'dog', 'point', 'piece'), 'made')\n",
      "Context 2: (('dog', 'made', 'piece', 'led'), 'point')\n",
      "Context 3: (('made', 'point', 'led', 'curate'), 'piece')\n",
      "Context 4: (('point', 'piece', 'curate', 'two'), 'led')\n"
     ]
    }
   ],
   "source": [
    "#Example: From Beginning of novel 1 to first context words\n",
    "\n",
    "print(f'Beginning of first novel: {novels[0][:62]}\\n')\n",
    "\n",
    "print(f'First tokens: {tokenized_novels[0][:8]}\\n')\n",
    "    \n",
    "print(f'Context 0: {contexts[0]}')\n",
    "print(f'Context 1: {contexts[1]}')\n",
    "print(f'Context 2: {contexts[2]}')\n",
    "print(f'Context 3: {contexts[3]}')\n",
    "print(f'Context 4: {contexts[4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45506821-a854-4a20-838d-643b5d0172b2",
   "metadata": {},
   "source": [
    "### Words to Index\n",
    "\n",
    "Unlike gensim's Words2vec, torch always requires indices instead of strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3acb32-160b-49aa-86f9-4b272f983c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <class 'torchtext.vocab.vocab.Vocab'> has indices for a total of 24.326 different words.\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_novels)\n",
    "\n",
    "print(f'The {type(vocab)} has indices for a total of {len(vocab) :n} different words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a8fdde-ac3f-472c-a433-a3e1ab4f5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: (('dog', 'made', 'piece', 'led'), 'point') -> ([935, 17, 949, 423], 321)\n"
     ]
    }
   ],
   "source": [
    "# convert our context training data's contents to indices\n",
    "context_indices: list[tuple[tuple[int, int, int, int], int]] = []\n",
    "for (context, target) in contexts:\n",
    "    context_ind = [vocab[c] for c in context]\n",
    "    target_ind = vocab[target]\n",
    "    context_indices.append((context_ind, target_ind))\n",
    "    \n",
    "print(f'Example: {contexts[2]} -> {context_indices[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a546be-1470-4e08-9ccf-96b50394ba7f",
   "metadata": {},
   "source": [
    "### Tensorize\n",
    "\n",
    "Finally, we need to tensorize our indices and targets.\n",
    "- Shape [185033, 4] for context words\n",
    "- Shape [185033] for target, i.e. value only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c5e135b-ba70-428a-b71c-874b0fdabd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = torch.Tensor([context_ind for (context_ind, _) in context_indices]).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c191ed31-6fb3-4458-ac11-d47b58fd3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets =  torch.Tensor([target_ind for (_, target_ind) in context_indices]).type(torch.long)\n",
    "\n",
    "assert len(contexts) == len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d331c-7edd-4709-aac4-4cf0e2df6539",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6227952c-1423-43c4-894a-eab3b9cec082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52907ebe-9b44-42a2-95c9-138578c78661",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Create a Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a044b9a-706a-4950-baa6-a312d1c868e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        assert len(x) == len(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "962a4b93-ab0a-4cce-be5f-eac85ba186e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(contexts.to(DEVICE), targets.to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7637a8-badc-49c3-b1bb-04d5bcfb4d5b",
   "metadata": {},
   "source": [
    "## Save and Load\n",
    "Preprocessing takes a long time when including all novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78986f46-849b-405b-bfc7-27fb46502693",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONTEXTS = BASE_PATH + 'saves/contexts_novel.pt'\n",
    "DATASET_TARGETS = BASE_PATH + 'saves/targets_novel.pt'\n",
    "VOCAB_PATH = BASE_PATH + 'saves/vocab.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e076fda-eaf6-4aa6-bddb-38d64180cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fcd98ba-6dfe-4c68-9a1c-6bf15a395dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(contexts, DATASET_CONTEXTS)\n",
    "torch.save(targets, DATASET_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9ce1801-bde4-45c1-bef6-e1863039cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = torch.load(VOCAB_PATH)\n",
    "#print(f'Loaded vocab of size {len(vocab) :n}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d58f66-52d2-4912-badb-691e0b4101de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save & load not the dataset but x and y to make transfer to device easier\n",
    "#contexts = torch.load(DATASET_CONTEXTS).to(DEVICE)\n",
    "#targets = torch.load(DATASET_TARGETS).to(DEVICE)\n",
    "#dataset = CustomDataset(contexts, targets)\n",
    "#print(f'Loaded {len(dataset) :n} context tensors as training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9938281c-490d-47ef-b72c-9f8131b2c381",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99fcd496-a76f-4a04-82e9-2873c9283dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59b3f46e-a567-4b49-9332-0a8e06a4e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49fa52-a617-42cb-8ddb-a3b5f69900dd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aa35808-63b1-4a06-a922-c981ee58bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,  # size of the dictionary of embeddings\n",
    "                                       embedding_dim=embedding_dim)  # size of each embedding vector\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):  # inputs: [16, 4]\n",
    "        embeds_ = self.embeddings(inputs)  # [16, 4, 100]\n",
    "        embeds = torch.sum(embeds_, dim=1)  # [16, 100]\n",
    "        # embeds = sum(self.embeddings(inputs)).view(1, -1)  # [1, 400]\n",
    "        out = self.linear1(embeds)  # [16, 128]\n",
    "        out = self.activation_function1(out)  # [16, 128]\n",
    "        out = self.linear2(out)  # [16, 20420]\n",
    "        out = self.activation_function2(out)  # [16, 20420]\n",
    "        return out\n",
    "\n",
    "    #def get_word_emdedding(self, word):\n",
    "    #    word = torch.tensor([word_to_ix[word]])\n",
    "    #    return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847803a0-e421-4b91-8c6c-0df0b5e7ada0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b77e890-a1b7-41c8-9e1d-48e5ce09854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9ee808c-a602-491a-9f8e-f2b5e2c38b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMDEDDING_DIM = 100\n",
    "\n",
    "N_EPOCHS = 1  # 50\n",
    "\n",
    "DISPLAY_EVERY_N_STEPS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65d80fcc-9e56-4a3b-a9a9-511435417e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(len(vocab), EMDEDDING_DIM).to(DEVICE)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8174b2c0-b7a0-4013-97fa-e89741c430a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with N_EPOCHS = 1 and a training data of 8.405.037 context tensors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efea8a6bbd834020bd3a01e63736b1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0/1  | batch 20/16.417  | 10.240/8.405.037 vectors done  | 5.64 sec.  | new lr 0.1 | loss 10.20\n",
      "| epoch   0/1  | batch 40/16.417  | 20.480/8.405.037 vectors done  | 4.79 sec.  | new lr 0.1 | loss 10.14\n",
      "| epoch   0/1  | batch 59/16.417  | 30.208/8.405.037 vectors done  | 4.53 sec.  | new lr 0.1 | loss 10.09\n",
      "| epoch   0/1  | batch 79/16.417  | 40.448/8.405.037 vectors done  | 4.70 sec.  | new lr 0.1 | loss 10.05\n",
      "| epoch   0/1  | batch 98/16.417  | 50.176/8.405.037 vectors done  | 4.44 sec.  | new lr 0.1 | loss 10.01\n",
      "| epoch   0/1  | batch 118/16.417  | 60.416/8.405.037 vectors done  | 4.67 sec.  | new lr 0.1 | loss  9.96\n",
      "| epoch   0/1  | batch 137/16.417  | 70.144/8.405.037 vectors done  | 4.45 sec.  | new lr 0.1 | loss  9.93\n",
      "| epoch   0/1  | batch 157/16.417  | 80.384/8.405.037 vectors done  | 4.80 sec.  | new lr 0.1 | loss  9.88\n",
      "| epoch   0/1  | batch 176/16.417  | 90.112/8.405.037 vectors done  | 4.64 sec.  | new lr 0.1 | loss  9.83\n",
      "| epoch   0/1  | batch 196/16.417  | 100.352/8.405.037 vectors done  | 4.94 sec.  | new lr 0.1 | loss  9.78\n",
      "| epoch   0/1  | batch 215/16.417  | 110.080/8.405.037 vectors done  | 4.72 sec.  | new lr 0.1 | loss  9.76\n",
      "| epoch   0/1  | batch 235/16.417  | 120.320/8.405.037 vectors done  | 4.95 sec.  | new lr 0.1 | loss  9.70\n",
      "| epoch   0/1  | batch 254/16.417  | 130.048/8.405.037 vectors done  | 4.70 sec.  | new lr 0.1 | loss  9.67\n",
      "| epoch   0/1  | batch 274/16.417  | 140.288/8.405.037 vectors done  | 5.00 sec.  | new lr 0.1 | loss  9.62\n",
      "| epoch   0/1  | batch 293/16.417  | 150.016/8.405.037 vectors done  | 4.70 sec.  | new lr 0.1 | loss  9.61\n",
      "| epoch   0/1  | batch 313/16.417  | 160.256/8.405.037 vectors done  | 5.00 sec.  | new lr 0.1 | loss  9.57\n",
      "| epoch   0/1  | batch 333/16.417  | 170.496/8.405.037 vectors done  | 5.05 sec.  | new lr 0.1 | loss  9.50\n",
      "| epoch   0/1  | batch 352/16.417  | 180.224/8.405.037 vectors done  | 4.81 sec.  | new lr 0.1 | loss  9.47\n",
      "| epoch   0/1  | batch 372/16.417  | 190.464/8.405.037 vectors done  | 5.35 sec.  | new lr 0.1 | loss  9.47\n",
      "| epoch   0/1  | batch 391/16.417  | 200.192/8.405.037 vectors done  | 4.91 sec.  | new lr 0.1 | loss  9.42\n",
      "| epoch   0/1  | batch 411/16.417  | 210.432/8.405.037 vectors done  | 5.14 sec.  | new lr 0.1 | loss  9.36\n",
      "| epoch   0/1  | batch 430/16.417  | 220.160/8.405.037 vectors done  | 4.89 sec.  | new lr 0.1 | loss  9.36\n",
      "| epoch   0/1  | batch 450/16.417  | 230.400/8.405.037 vectors done  | 5.22 sec.  | new lr 0.1 | loss  9.32\n",
      "| epoch   0/1  | batch 469/16.417  | 240.128/8.405.037 vectors done  | 4.89 sec.  | new lr 0.1 | loss  9.30\n",
      "| epoch   0/1  | batch 489/16.417  | 250.368/8.405.037 vectors done  | 5.10 sec.  | new lr 0.1 | loss  9.27\n",
      "| epoch   0/1  | batch 508/16.417  | 260.096/8.405.037 vectors done  | 4.52 sec.  | new lr 0.1 | loss  9.28\n",
      "| epoch   0/1  | batch 528/16.417  | 270.336/8.405.037 vectors done  | 4.82 sec.  | new lr 0.1 | loss  9.21\n",
      "| epoch   0/1  | batch 547/16.417  | 280.064/8.405.037 vectors done  | 4.59 sec.  | new lr 0.1 | loss  9.21\n",
      "| epoch   0/1  | batch 567/16.417  | 290.304/8.405.037 vectors done  | 4.85 sec.  | new lr 0.1 | loss  9.19\n",
      "| epoch   0/1  | batch 586/16.417  | 300.032/8.405.037 vectors done  | 5.32 sec.  | new lr 0.010000000000000002 | loss  9.17\n",
      "| epoch   0/1  | batch 606/16.417  | 310.272/8.405.037 vectors done  | 6.96 sec.  | new lr 0.010000000000000002 | loss  9.16\n",
      "| epoch   0/1  | batch 625/16.417  | 320.000/8.405.037 vectors done  | 5.74 sec.  | new lr 0.010000000000000002 | loss  9.14\n",
      "| epoch   0/1  | batch 645/16.417  | 330.240/8.405.037 vectors done  | 5.17 sec.  | new lr 0.010000000000000002 | loss  9.15\n",
      "| epoch   0/1  | batch 665/16.417  | 340.480/8.405.037 vectors done  | 5.46 sec.  | new lr 0.010000000000000002 | loss  9.17\n",
      "| epoch   0/1  | batch 684/16.417  | 350.208/8.405.037 vectors done  | 5.30 sec.  | new lr 0.010000000000000002 | loss  9.16\n",
      "| epoch   0/1  | batch 704/16.417  | 360.448/8.405.037 vectors done  | 5.50 sec.  | new lr 0.010000000000000002 | loss  9.14\n",
      "| epoch   0/1  | batch 723/16.417  | 370.176/8.405.037 vectors done  | 5.07 sec.  | new lr 0.010000000000000002 | loss  9.17\n",
      "| epoch   0/1  | batch 743/16.417  | 380.416/8.405.037 vectors done  | 5.42 sec.  | new lr 0.010000000000000002 | loss  9.16\n",
      "| epoch   0/1  | batch 762/16.417  | 390.144/8.405.037 vectors done  | 5.20 sec.  | new lr 0.010000000000000002 | loss  9.16\n",
      "| epoch   0/1  | batch 782/16.417  | 400.384/8.405.037 vectors done  | 5.72 sec.  | new lr 0.010000000000000002 | loss  9.14\n",
      "| epoch   0/1  | batch 801/16.417  | 410.112/8.405.037 vectors done  | 5.15 sec.  | new lr 0.010000000000000002 | loss  9.13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [16, 20420]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     current_loss \u001b[38;5;241m=\u001b[39m loss_function(log_probs, y_batch)  \u001b[38;5;66;03m# [16]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     recent_losses\u001b[38;5;241m.\u001b[39mappend(current_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mC:\\workspaces\\pycharm_projects\\simple-gans\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[29], line 23\u001b[0m, in \u001b[0;36mCBOW.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(embeds)  \u001b[38;5;66;03m# [16, 128]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function1(out)  \u001b[38;5;66;03m# [16, 128]\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [16, 20420]\u001b[39;00m\n\u001b[0;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function2(out)  \u001b[38;5;66;03m# [16, 20420]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\workspaces\\pycharm_projects\\simple-gans\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\workspaces\\pycharm_projects\\simple-gans\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Starting training with N_EPOCHS = {N_EPOCHS} and a training data of {len(contexts) :n} context tensors.')\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "total_steps = N_EPOCHS * len(train_loader.dataset)\n",
    "recent_losses = []\n",
    "interim_time = time.time()\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # x_batch: [{batch_size}, 4]\n",
    "    # y_batch: [{batch_size}]\n",
    "    for batch, (x_batch, y_batch) in enumerate(tqdm(train_loader)):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_probs = model(x_batch)  # [16, 20420]\n",
    "        current_loss = loss_function(log_probs, y_batch)  # [16]\n",
    "\n",
    "        recent_losses.append(current_loss.item())\n",
    "\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        steps_done += len(x_batch)\n",
    "        \n",
    "        if (steps_done) % DISPLAY_EVERY_N_STEPS < len(x_batch):\n",
    "            elapsed_time = time.time() - interim_time\n",
    "            interim_time = time.time()\n",
    "            average_loss = np.average(recent_losses)\n",
    "            recent_losses = []\n",
    "            scheduler.step()\n",
    "            print(f'| epoch {epoch + 1 :3d}/{N_EPOCHS} ',\n",
    "                  f'| batch {batch + 1 :n}/{len(train_loader) :n} ',\n",
    "                  f'| {steps_done :n}/{total_steps :n} vectors done ',\n",
    "                  f'| {elapsed_time :.2f} sec. ',\n",
    "                  f'| new lr {optimizer.param_groups[0][\"lr\"]}',\n",
    "                  f'| loss {average_loss:5.2f}') #  :n\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf3db290-b5e0-4515-bea3-3da6908eedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "MODEL_PATH = BASE_PATH + 'saves/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7120539f-b224-406c-80e3-747a6e957953",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369f6bd-0c04-41f1-a18d-e72c581d7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "#model = CBOW(len(vocab), EMDEDDING_DIM)\n",
    "#model.load_state_dict(torch.load(MODEL_PATH))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70660dd8-2a1d-421d-9372-eadded45431e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "862faffd-124e-4a2b-911a-446b785e0323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(24326, 100)\n",
       "  (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (activation_function1): ReLU()\n",
       "  (linear2): Linear(in_features=128, out_features=24326, bias=True)\n",
       "  (activation_function2): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98dc4338-0ae6-40dd-900e-a1e0d4d1a891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_index(*tokens) -> list[int] | int:\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            raise ValueError(f'Token not found: {token}')\n",
    "        indices.append(vocab[token])\n",
    "    return indices if len(indices) > 1 else indices[0]\n",
    "\n",
    "get_index('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99177644-e252-4a1c-9489-312ae6c0f0e2",
   "metadata": {},
   "source": [
    "### Normalize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1baecbb3-5977-4505-ae90-c08cef7a623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read embedding from first model layer\n",
    "embeddings = next(model.embeddings.parameters()).cpu().detach().numpy()  # (16390, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f9dfa9f-0988-4120-a89d-211fa015efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)  # ndarray (16390,)\n",
    "norms = np.reshape(norms, (len(norms), 1))  # (16390, 1)\n",
    "embeddings_normalized = embeddings / norms  # (16390, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bc79d-93f1-4371-ae3d-6e11f3190b53",
   "metadata": {},
   "source": [
    "### Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5ebaeed-a259-4b30-9612-e785bb0d78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar(word: str, top_n: int):\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f'Not found: {word}')\n",
    "    word_index = vocab[word]\n",
    "\n",
    "    word_vector = embeddings_normalized[word_index]\n",
    "    word_vector = np.reshape(word_vector, (len(word_vector), 1))\n",
    "    distances = np.matmul(embeddings_normalized, word_vector).flatten()\n",
    "    top_n_indices = np.argsort(-distances)[1 : top_n + 1]  # the nearest is always the word itself\n",
    "\n",
    "    top_n_dict = {}\n",
    "    for similar_word_index in top_n_indices:\n",
    "        similar_word = vocab.lookup_token(similar_word_index)\n",
    "        top_n_dict[similar_word] = distances[similar_word_index]\n",
    "    return top_n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9af0076d-e00c-4440-a91b-9bc733d76043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backhouse           : 0.424\n",
      "samuel              : 0.375\n",
      "afraid              : 0.356\n",
      "frockcoat           : 0.353\n",
      "isabella            : 0.353\n",
      "feelingly           : 0.346\n",
      "amory               : 0.345\n",
      "maturity            : 0.345\n",
      "uniformity          : 0.332\n",
      "extenuate           : 0.331\n"
     ]
    }
   ],
   "source": [
    "for word, similarity in get_top_similar(\"king\", top_n=10).items():\n",
    "    print(f\"{word: <20}: {similarity :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5da19-8dbd-4ff1-b749-dcc013643aa9",
   "metadata": {},
   "source": [
    "### Vector Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a91fc145-51bf-4821-80c0-e960e418f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman: 0.667\n",
      "king: 0.599\n",
      "fright: 0.346\n",
      "signal: 0.345\n",
      "rival: 0.326\n"
     ]
    }
   ],
   "source": [
    "emb1 = embeddings[vocab[\"king\"]]\n",
    "emb2 = embeddings[vocab[\"man\"]]\n",
    "emb3 = embeddings[vocab[\"woman\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
    "emb4 = emb4 / emb4_norm\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings_normalized, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b144e-b39c-412e-a8e8-543eb6b37653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
