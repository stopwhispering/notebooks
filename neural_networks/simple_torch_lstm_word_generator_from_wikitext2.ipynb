{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda3705a-fcbf-4200-bbad-3903ed9c8319",
   "metadata": {},
   "source": [
    "# PyTorchðŸ”¥Words Prediction Demo from Wikitext2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d99ab-9f5a-435b-b508-4345cb0f8921",
   "metadata": {},
   "source": [
    "Subject: Building a demo word prediction model.\n",
    "\n",
    "Data: WikiText-2 via torchtext \n",
    "\n",
    "Procedure:\n",
    "- Creating a vocabulary as simple set of distinct words\n",
    "- Tokenizing simply via '...'.split()\n",
    "- Creating a tensor from the indices of the flat word list\n",
    "  Train is tensor of indices of 35 subsequent words, Target is the same but with one word further. (e.g. 0, 1, ..., 34 -> 1, 2, ..., 35)\n",
    "- RNN Model with Long short-term memory (LSTM) and embeddings layer. Using torch.nn.Dropout, torch.nn.Embedding, torch.nn.LSTM, torch.nn.Linear, and torch.nn.functional.log_softmax.\n",
    "- Training with torch.nn.NLLLoss, no optimizer (?)\n",
    "- Evaluation by generating some words\n",
    "- Disappointing results (to be expected with that small dataset)\n",
    "\n",
    "Others:\n",
    "- CUDA support\n",
    "- working on Colab with Google Drive for saving/loading interim stages\n",
    "\n",
    "Sources used:\n",
    "- https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a690c5-f496-44d1-bcea-cf4b5231060b",
   "metadata": {},
   "source": [
    "## Bootstrap and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "266bba17-c2e1-4868-86f0-9a3f536c9e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {DEVICE}')\n",
    "\n",
    "if IN_COLAB := 'google.colab' in str(get_ipython()):\n",
    "  NUM_EPOCHS = 40\n",
    "  !pip install torchviz\n",
    "  !pip install portalocker\n",
    "\n",
    "else:\n",
    "  NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8486b94-aaf9-4c9a-8426-4f189c245284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e9ee7af-c74b-4726-b6c9-1153d33fb9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f0d7db0a70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from collections.abc import Callable\n",
    "import os\n",
    "import math\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "import time\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_seed = 123\n",
    "random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acb34d-9d77-42a9-83a8-7387e4c0581d",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b205cf4-e146-4d74-b2f0-62164b6b2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "BATCH_SIZE_EVALUATION = 10\n",
    "LEARNING_RATE = 20\n",
    "SEQUENCE_LENGTH = 35\n",
    "GRADIENT_CLIPPING = 0.25\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "DRY_RUN = False  # verify the code and the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50d31c-2518-4d2c-a9dc-4e258e7cbcd5",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf5fc82-5cfe-4d15-b1ec-49c06dbfb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "        \n",
    "class Corpus(object):\n",
    "    def __init__(self, device):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(torchtext.datasets.WikiText2(split='train')).to(device)\n",
    "        self.valid = self.tokenize(torchtext.datasets.WikiText2(split='valid')).to(device)\n",
    "        self.test = self.tokenize(torchtext.datasets.WikiText2(split='test')).to(device)\n",
    "    \n",
    "    def tokenize(self, iter):\n",
    "        # Add words to the dictionary\n",
    "        for line in iter:\n",
    "            words = line.split() + ['<eos>']\n",
    "            for word in words:\n",
    "                self.dictionary.add_word(word)\n",
    "\n",
    "        idss = []\n",
    "        for line in iter:\n",
    "            words = line.split() + ['<eos>']\n",
    "            ids = []\n",
    "            for word in words:\n",
    "                ids.append(self.dictionary.word2idx[word])\n",
    "            idss.append(torch.tensor(ids).type(torch.int64))\n",
    "        ids = torch.cat(idss)  # [2088628] / [217646] / [245569]\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8be941bb-9fc4-4885-bee0-9debdfbfc575",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c325312b-3236-499c-a9a6-fd80abd11779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# â”Œ a g m s â”\n",
    "# â”‚ b h n t â”‚\n",
    "# â”‚ c i o u â”‚\n",
    "# â”‚ d j p v â”‚\n",
    "# â”‚ e k q w â”‚\n",
    "# â”” f l r x â”˜.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data: torch.Tensor, batch_size: int) -> torch.Tensor:\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac1fd80-cfa8-4e8d-b1bf-10561dae8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, BATCH_SIZE)  # [104431, 20]  torch.int64\n",
    "val_data = batchify(corpus.valid, BATCH_SIZE_EVALUATION)  # [21764, 10]  torch.int64\n",
    "test_data = batchify(corpus.test, BATCH_SIZE_EVALUATION)  # [24556, 10]  torch.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41505ef0-1fe3-4da0-8f2a-d8bd95e56dde",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0a46629-d98a-4846-a6d2-08d413d64c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 ntoken, \n",
    "                 rnn_type='LSTM', \n",
    "                 ninp=200,  # size of word embeddings \n",
    "                 nhid=200,  # number of hidden units per layer \n",
    "                 nlayers=2, \n",
    "                 dropout=0.2, \n",
    "                 tie_weights=True,  # tie the word embedding and softmax weights\n",
    "                ):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(num_embeddings=ntoken,   # size of the dictionary of embeddings\n",
    "                                    embedding_dim=ninp)  # size of each embedding vector\n",
    "        \n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ce591d-3509-4551-8a66-72b459add141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntoken=ntokens).to(DEVICE).to(DEVICE)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb35739-aa9d-453e-88a4-b4906168cf26",
   "metadata": {},
   "source": [
    "## Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28a8d7ee-a70f-48ba-9769-683141eeb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "703143b7-2a94-426c-b8bb-aa6e6c3d548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length SEQUENCE_LENGTH.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# â”Œ a g m s â” â”Œ b h n t â”\n",
    "# â”” b h n t â”˜ â”” c i o u â”˜\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(SEQUENCE_LENGTH, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "825d85fb-ed64-4c68-815a-8f89d50a472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(BATCH_SIZE_EVALUATION)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, SEQUENCE_LENGTH):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6a790-b6c3-4a86-8a57-81430921127e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6460e20d-bf7b-4c36-a8f8-9cfe27dd3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, SEQUENCE_LENGTH)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-LEARNING_RATE)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // SEQUENCE_LENGTH, LEARNING_RATE,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        if DRY_RUN:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc8446a-2413-4290-92fa-1e6ab1ac78d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    10/ 2983 batches | lr 20.00 | ms/batch 409.94 | loss 10.56 | ppl 38413.97\n",
      "| epoch   0 |    20/ 2983 batches | lr 20.00 | ms/batch 360.66 | loss  8.30 | ppl  4026.61\n",
      "| epoch   0 |    30/ 2983 batches | lr 20.00 | ms/batch 349.42 | loss  8.00 | ppl  2966.53\n",
      "| epoch   0 |    40/ 2983 batches | lr 20.00 | ms/batch 359.22 | loss  7.82 | ppl  2499.71\n",
      "| epoch   0 |    50/ 2983 batches | lr 20.00 | ms/batch 346.90 | loss  7.73 | ppl  2264.96\n",
      "| epoch   0 |    60/ 2983 batches | lr 20.00 | ms/batch 337.15 | loss  7.69 | ppl  2177.92\n",
      "| epoch   0 |    70/ 2983 batches | lr 20.00 | ms/batch 341.78 | loss  7.58 | ppl  1952.22\n",
      "| epoch   0 |    80/ 2983 batches | lr 20.00 | ms/batch 342.28 | loss  7.53 | ppl  1865.86\n",
      "| epoch   0 |    90/ 2983 batches | lr 20.00 | ms/batch 349.00 | loss  7.49 | ppl  1798.14\n",
      "| epoch   0 |   100/ 2983 batches | lr 20.00 | ms/batch 343.89 | loss  7.50 | ppl  1800.64\n",
      "| epoch   0 |   110/ 2983 batches | lr 20.00 | ms/batch 346.67 | loss  7.37 | ppl  1582.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m      2\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(val_data)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m89\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m model(data, hidden)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, targets)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), GRADIENT_CLIPPING)\n",
      "File \u001b[1;32mC:\\workspaces\\pycharm_projects\\simple-gans\\.venv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspaces\\pycharm_projects\\simple-gans\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(val_data)\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {(time.time() - epoch_start_time):5.2f}s | valid loss {val_loss:5.2f} | '\n",
    "          f'  valid ppl {math.exp(val_loss):8.2f}')\n",
    "    print('-' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadac596-e2a8-4916-9ad1-b8bdc54b310a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9deef-9737-4227-8896-673c9518807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss: 5.2f} | test ppl {math.exp(test_loss): 8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b2c93-a59e-4095-942d-35aff6bc2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680fffa-c6c0-4459-9ef3-c2e8c0f5c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'| End of training | test loss {test_loss: 5.2f} | test ppl {math.exp(test_loss): 8.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed5dad-5534-471c-94b0-3a9bb0c72d55",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae47ac-baed-45b7-afc9-d0a785ce3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_COUNT = 1000  # number of words to generate\n",
    "LOG_INTERVAL_GENERATION = 100\n",
    "TEMPERATURE = 1.0  # higher will increase diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f2b4b-9da1-40b2-92db-8cd983aaa514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(DEVICE)\n",
    "\n",
    "words_output = ''\n",
    "\n",
    "with torch.no_grad():  # no tracking history\n",
    "    for i in range(WORDS_COUNT):\n",
    "\n",
    "        output, hidden = model(input, hidden)\n",
    "        word_weights = output.squeeze().div(TEMPERATURE).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        input.fill_(word_idx)\n",
    "\n",
    "        word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "        words_output += (word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "        if i % LOG_INTERVAL_GENERATION == 0:\n",
    "            print('| Generated {}/{} words'.format(i, WORDS_COUNT))\n",
    "\n",
    "print(words_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa9a06-306e-4953-82c1-0b0296898429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
