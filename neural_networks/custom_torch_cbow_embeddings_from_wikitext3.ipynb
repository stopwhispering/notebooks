{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d542ff-4edf-4416-bc7e-f899c807b2b6",
   "metadata": {},
   "source": [
    "# PyTorch🔥Embeddings - Build Word2vec CBOW Model (wikitext3 Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2592e2-8e2a-425a-b006-5d06149d105b",
   "metadata": {},
   "source": [
    "Subject: Building a Word2vec-like CBOW Model to create embeddings for the dataset's words representing their near closeness to each other (in 300 embedding dimensions). Preprocessing fails on Colab due to data size, therefore we process data as <b>iterators</b>, hot having the whole dataset in memory at any time.\n",
    "\n",
    "Data: Wikitext3 from torch as DataPipe (beta)\n",
    "\n",
    "Procedure:\n",
    "\n",
    "- Creating a vocabulary with torchtext.vocab.build_vocab_from_iterator\n",
    "- Tokenizing with torchtext.data.utils.get_tokenizer and nltk.corpus.stopwords\n",
    "- Creating contexts and targets from five words each: (01 34) with (2) as target\n",
    "- Tensorizing contexts and targets\n",
    "- Creating a custom torch.utils.data.Dataset for a torch.utils.data.DataLoader\n",
    "- Word2vec-like CBOW model with torch.nn.module, torch.nn.Embedding, torch.nn.Linear, torch.nn.ReLU, and torch.nn.LogSoftmax\n",
    "- Training with torch.nn.NLLLoss, torch.optim.SGD, and torch.optim.lr_scheduler.StepLR\n",
    "- Evaluation by finding some nearest words and playing with word vectors\n",
    "- Disappointing results (probably much more data required)\n",
    "\n",
    "Others:\n",
    "- CUDA support\n",
    "- working on Colab with Google Drive for saving/loading interim stages\n",
    "\n",
    "Sources used:\n",
    "- https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f73ae1-5184-442f-85b1-f3740caf666e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f781709e-11ba-46b7-b3f0-3f25fbd9d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {DEVICE}')\n",
    "\n",
    "if IN_COLAB := 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  BASE_PATH = './drive/MyDrive/Colab/'\n",
    "  import nltk\n",
    "  nltk.download('stopwords')\n",
    "\n",
    "else:\n",
    "  BASE_PATH = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f21b1-93ad-4999-9709-d598d3c98024",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8269153-8280-4ae6-b361-bc069e676623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "import torchtext\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, locale='de_DE.utf8')  # for thousands separator via ... print(f'{value:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ef7d76-5cb0-4846-951e-253872d9e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_FREQUENCY = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827578b4-7024-4f4c-b5de-81b6b1882fd2",
   "metadata": {},
   "source": [
    "### Load wikitext3 as DataPipe (Iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa71af3-6fc6-4232-a159-07d2281f6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.datasets.WikiText103(split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88474bf-fa74-4bdc-b821-6bd9c3eebde1",
   "metadata": {},
   "source": [
    "### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1915d494-b3e8-49a6-bbcf-24ef6a3fc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies used for both building vocabulary and processing wikipedia texts in training\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "translator = str.maketrans('', '', string.punctuation + '“”’…')  # —"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2ffaf8-8dab-4afc-8cdf-af9ba54b9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer_fn(row: str) -> list[str]:\n",
    "    row_ = row.translate(translator)\n",
    "    tokens = tokenizer(row_.lower())\n",
    "    tokens_ = [token for token in tokens if token not in stop_words]\n",
    "    tokens__ = [word for word in tokens_ if not re.findall(pattern='[^A-Za-z0-9.]+', string=word)]\n",
    "    return tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decdb9ad-fdf9-44ef-b641-1f16f9099351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workspaces\\pycharm_projects\\simple-gans\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 25814 tokens.\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    iterator=map(custom_tokenizer_fn, train_iter),  # Must yield list or iterator of tokens\n",
    "    min_freq=MIN_WORD_FREQUENCY\n",
    "    )\n",
    "\n",
    "# vocab = torch.load(BASE_PATH + 'vocab_wikitext3.pt')\n",
    "torch.save(vocab, BASE_PATH + 'vocab_wikitext3.pt')\n",
    "print(f'Vocabulary has {len(vocab)} tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0969621-bd36-4c3a-bfe2-fe8968f5c4c2",
   "metadata": {},
   "source": [
    "### WikipediaProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f42baaa-91e6-4b64-a2fe-e83b4fa183c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaProcessor:\n",
    "    def __init__(self,\n",
    "                 vocab: torchtext.vocab.Vocab,\n",
    "                 stop_words: set[str],\n",
    "                 translator: dict,\n",
    "                 device: str):\n",
    "        self.vocab = vocab\n",
    "        self.stop_words = stop_words\n",
    "        self.translator = translator\n",
    "        self.device = device\n",
    "\n",
    "    def process(self, paragraphs: list[str]):\n",
    "\n",
    "        contexts_as_indices = []\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            tokenized = self._tokenize(paragraph)\n",
    "            if not tokenized:\n",
    "                continue\n",
    "            contexts = self._create_contexts(tokenized)\n",
    "            if not contexts:\n",
    "                continue\n",
    "            contexts_as_indices.extend(self._to_indices(contexts))\n",
    "\n",
    "        contexts, targets = self._tensorize(contexts_as_indices)\n",
    "        return contexts, targets\n",
    "\n",
    "    def _tensorize(self, context_indices: list[tuple[tuple[int, int, int, int], int]]\n",
    "                   ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        contexts = torch.Tensor(\n",
    "            [context_ind for (context_ind, _) in context_indices]\n",
    "        ).type(torch.long).to(self.device)\n",
    "\n",
    "        targets = torch.Tensor([target_ind for (_, target_ind) in context_indices]).type(\n",
    "            torch.long).to(self.device)\n",
    "        assert len(contexts) == len(targets)\n",
    "        return contexts, targets\n",
    "\n",
    "    def _tokenize(self, paragraph: str) -> list[str]:\n",
    "        without_punctuation = paragraph.translate(self.translator)\n",
    "        tokenized_1 = tokenizer(without_punctuation.lower())\n",
    "        tokenized_2 = [token for token in tokenized_1 if token not in self.stop_words]\n",
    "        tokenized_3 = [word for word in tokenized_2 if\n",
    "                       not re.findall(pattern='[^A-Za-z0-9.]+', string=word)]\n",
    "        return [word for word in tokenized_3 if word in self.vocab]\n",
    "\n",
    "    def _create_contexts(self, tokenized_paragraph: list[str]\n",
    "                        ) -> list[tuple[tuple[str, str, str, str], str]]:\n",
    "        contexts = []\n",
    "        for i in range(2, len(tokenized_paragraph) - 2):\n",
    "            context = (tokenized_paragraph[i - 2],\n",
    "                       tokenized_paragraph[i - 1],\n",
    "                       tokenized_paragraph[i + 1],\n",
    "                       tokenized_paragraph[i + 2])\n",
    "            target = tokenized_paragraph[i]\n",
    "            contexts.append((context, target))\n",
    "        return contexts\n",
    "\n",
    "    def _to_indices(self, contexts: list[tuple[tuple[str, str, str, str], str]]\n",
    "                   ) -> list[tuple[tuple[int, ...], int]]:\n",
    "        context_indices = []\n",
    "        for (context, target) in contexts:\n",
    "            context_ind = tuple(self.vocab[c] for c in context)\n",
    "            target_ind = self.vocab[target]\n",
    "            context_indices.append((context_ind, target_ind))\n",
    "        return context_indices\n",
    "\n",
    "\n",
    "wikipedia_processor = WikipediaProcessor(vocab=vocab,\n",
    "                                         stop_words=stop_words,\n",
    "                                         translator=translator,\n",
    "                                         device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f3ed6-5001-480f-8ca4-0618a3b3f81c",
   "metadata": {},
   "source": [
    "## DataLoader for wikitext3 DataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a452a1fb-292e-4894-a61a-06a366fa2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ccdf483-9f42-4603-95e8-d9fe6330c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # num of wikipedia paragraphs per training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20508103-48eb-40e8-8ade-50d060a52891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                   dataset=train_iter,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509fa111-3233-4e26-8f71-cc0cb6e49228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be2897d-c80a-43a6-8992-2de08ef011e9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d09821b0-6a55-499f-be12-592d5b073504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d8804b4-09e1-4d96-a4b6-55e6ef3f3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,  # size of the dictionary of embeddings\n",
    "                                       embedding_dim=embedding_dim)  # size of each embedding vector\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):  # inputs: [16, 4]\n",
    "        embeds_ = self.embeddings(inputs)  # [16, 4, 100]\n",
    "        embeds = torch.sum(embeds_, dim=1)  # [16, 100]\n",
    "        # embeds = sum(self.embeddings(inputs)).view(1, -1)  # [1, 400]\n",
    "        out = self.linear1(embeds)  # [16, 128]\n",
    "        out = self.activation_function1(out)  # [16, 128]\n",
    "        out = self.linear2(out)  # [16, 20420]\n",
    "        out = self.activation_function2(out)  # [16, 20420]\n",
    "        return out\n",
    "\n",
    "    #def get_word_emdedding(self, word):\n",
    "    #    word = torch.tensor([word_to_ix[word]])\n",
    "    #    return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40c330-38dc-46b9-85f2-a152435f8576",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "220ca132-aafc-450b-89bb-5b5163566728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37e9a2e8-5808-4ccf-a3e6-eb4a3d29e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMDEDDING_DIM = 300\n",
    "\n",
    "N_EPOCHS = 1  # 50\n",
    "\n",
    "UPDATE_LR_EVERY_N_EPOCHS = 2\n",
    "\n",
    "DISPLAY_EVERY_N_STEPS = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9403d54-8903-4ee4-b089-2173deb698f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(len(vocab), EMDEDDING_DIM).to(DEVICE)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=UPDATE_LR_EVERY_N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb063f-1fbc-4cb9-a4be-1f3587361011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "total_steps_done = 0\n",
    "TRAIN_SIZE = 1801350  # len(train_loader.dataset)  this DataPipe is buggy\n",
    "total_steps = N_EPOCHS * TRAIN_SIZE\n",
    "recent_losses = []\n",
    "interim_time = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_steps_done = 0\n",
    "\n",
    "    progress_bar = tqdm(total=TRAIN_SIZE)\n",
    "\n",
    "    # x_batch: [{batch_size}, 4]\n",
    "    # y_batch: [{batch_size}]\n",
    "    for batch_number, train_batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        x_train, targets = wikipedia_processor.process(train_batch)\n",
    "\n",
    "        if len(x_train) == 0:\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_probs = model(x_train)  # [16, 20420]\n",
    "        current_loss = loss_function(log_probs, targets)  # [16]\n",
    "\n",
    "        recent_losses.append(current_loss.item())\n",
    "\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_steps_done += len(train_batch)\n",
    "        epoch_steps_done += len(train_batch)\n",
    "        progress_bar.update(epoch_steps_done)\n",
    "        \n",
    "        if (total_steps_done) % DISPLAY_EVERY_N_STEPS < len(train_batch):\n",
    "            elapsed_time = time.time() - interim_time\n",
    "            interim_time = time.time()\n",
    "            average_loss = np.average(recent_losses)\n",
    "            recent_losses = []\n",
    "            print(f'| epoch {epoch + 1 :3d}/{N_EPOCHS} ',\n",
    "                  f'| batch {batch_number + 1 :n}/{TRAIN_SIZE :n} ',\n",
    "                  f'| {total_steps_done :n}/{total_steps :n} vectors done ',\n",
    "                  f'| {elapsed_time :.2f} sec. ',\n",
    "                  f'| lr {optimizer.param_groups[0][\"lr\"]}',\n",
    "                  f'| loss {average_loss:5.2f}') #  :n\n",
    "            \n",
    "    scheduler.step()\n",
    "    torch.save(model.state_dict(), BASE_PATH + 'saves/model_wikitext3.pt')\n",
    "    progress_bar.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c695cb-6d2b-4b00-9d1d-e866c36d7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_PATH + 'saves/model_wikitext3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29f748-3fa9-4c06-8e5f-8a9988fe14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "#model = CBOW(len(vocab), EMDEDDING_DIM)\n",
    "#model.load_state_dict(torch.load(BASE_PATH + 'saves/saves/model_wikitext3.pt'))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591899a9-5b39-4c4b-8394-f043a4076f33",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe81016-76cc-4140-aaca-265191fd6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ced29b-f994-4918-934c-aa422723cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(*tokens) -> list[int] | int:\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            raise ValueError(f'Token not found: {token}')\n",
    "        indices.append(vocab[token])\n",
    "    return indices if len(indices) > 1 else indices[0]\n",
    "\n",
    "get_index('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80092ce4-3796-4d2f-8fda-619622b9e781",
   "metadata": {},
   "source": [
    "### Normalize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0846b-0e65-49c1-aeb0-1dc5915ebc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read embedding from first model layer\n",
    "embeddings = next(model.embeddings.parameters()).cpu().detach().numpy()  # (16390, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d654e-b161-4c86-bcff-f887a8955547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)  # ndarray (16390,)\n",
    "norms = np.reshape(norms, (len(norms), 1))  # (16390, 1)\n",
    "embeddings_normalized = embeddings / norms  # (16390, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e19a44-43d5-4ba8-9e21-31b4da53852c",
   "metadata": {},
   "source": [
    "### Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85ad7c-e8af-4cd3-abd1-5262e71145e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar(word: str, top_n: int):\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f'Not found: {word}')\n",
    "    word_index = vocab[word]\n",
    "\n",
    "    word_vector = embeddings_normalized[word_index]\n",
    "    word_vector = np.reshape(word_vector, (len(word_vector), 1))\n",
    "    distances = np.matmul(embeddings_normalized, word_vector).flatten()\n",
    "    top_n_indices = np.argsort(-distances)[1 : top_n + 1]  # the nearest is always the word itself\n",
    "\n",
    "    top_n_dict = {}\n",
    "    for similar_word_index in top_n_indices:\n",
    "        similar_word = vocab.lookup_token(similar_word_index)\n",
    "        top_n_dict[similar_word] = distances[similar_word_index]\n",
    "    return top_n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81b768-b192-45f8-9daf-bb743d3851e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, similarity in get_top_similar(\"king\", top_n=10).items():\n",
    "    print(f\"{word: <20}: {similarity :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9b60e-4274-4032-8910-7d49e7ea6ee5",
   "metadata": {},
   "source": [
    "### Vector Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2a958-b8e2-486b-85ff-60851beb9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = embeddings[vocab[\"king\"]]\n",
    "emb2 = embeddings[vocab[\"man\"]]\n",
    "emb3 = embeddings[vocab[\"woman\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
    "emb4 = emb4 / emb4_norm\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings_normalized, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
